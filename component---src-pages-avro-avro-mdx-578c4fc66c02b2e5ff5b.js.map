{"version":3,"sources":["webpack:///./src/pages/avro/avro.mdx"],"names":["_frontmatter","layoutProps","MDXLayout","DefaultLayout","MDXContent","components","props","mdxType","parentName","isMDXComponent"],"mappings":"0eAMO,IAAMA,EAAe,GAOtBC,EAAc,CAClBD,gBAEIE,EAAYC,IACH,SAASC,EAAT,GAGZ,IAFDC,EAEC,EAFDA,WACGC,E,oIACF,mBACD,OAAO,YAACJ,EAAD,KAAeD,EAAiBK,EAAhC,CAAuCD,WAAYA,EAAYE,QAAQ,cAG5E,sCACA,4CAA2B,mBAAGC,WAAW,KAAQ,CAC7C,KAAQ,6BADe,eAA3B,+EAEwG,sBAAQA,WAAW,KAAnB,2BAFxG,oCAEwM,mBAAGA,WAAW,KAAQ,CAC1N,KAAQ,yDAD4L,8CAFxM,WAImE,mBAAGA,WAAW,KAAQ,CACrF,KAAQ,0DADuD,yDAJnE,utBAOA,6CACA,yRACA,yCACA,mPAAkO,mBAAGA,WAAW,KAAQ,CACpP,KAAQ,mDADsN,uBAAlO,KAGA,sBACE,kBAAIA,WAAW,MAAf,4CACA,kBAAIA,WAAW,MAAf,8JACA,kBAAIA,WAAW,MAAf,oBACA,kBAAIA,WAAW,MAAf,iPACA,kBAAIA,WAAW,MAAf,kEACA,kBAAIA,WAAW,MAAf,8EAEF,yDACA,8hBACA,0WAAyV,sBAAQA,WAAW,KAAnB,aAAzV,8KACA,kGAAiF,0BAAYA,WAAW,KAAvB,sBAAjF,aAA6J,0BAAYA,WAAW,KAAvB,YAA7J,gRAAke,0BAAYA,WAAW,KAAvB,YAAle,sMAA6tB,0BAAYA,WAAW,KAAvB,YAA7tB,wFAA02B,0BAAYA,WAAW,KAAvB,YAA12B,sLACA,8CACA,mBAAU,CACR,UAAa,4BACb,MAAS,CACP,SAAY,WACZ,QAAW,QACX,WAAc,OACd,YAAe,OACf,SAAY,WAPhB,WAUI,sBAAMA,WAAW,QAAW,CAC5B,UAAa,qCACb,MAAS,CACP,cAAiB,qBACjB,SAAY,WACZ,OAAU,IACV,KAAQ,IACR,gBAAmB,gwBACnB,eAAkB,QAClB,QAAW,YAnBjB,OAsBA,qBAAKA,WAAW,QAAW,CACvB,UAAa,0BACb,IAAO,6BACP,MAAS,6BACT,IAAO,0FACP,OAAU,CAAC,+FAAgG,+FAAgG,gGAAiG,iGAC5S,MAAS,oCACT,MAAS,CACP,MAAS,OACT,OAAU,OACV,OAAU,IACV,cAAiB,SACjB,SAAY,WACZ,IAAO,IACP,KAAQ,KAEV,QAAW,UAtCf,UAyCA,2SAA0R,sBAAQA,WAAW,KAAnB,aAA1R,kCAA0W,sBAAQA,WAAW,KAAnB,wDAA1W,sCAAye,sBAAQA,WAAW,KAAnB,qEAAze,+MAA8xB,sBAAQA,WAAW,KAAnB,aAA9xB,6CACA,qEAAoD,sBAAQA,WAAW,KAAnB,gEAApD,4EAAiO,sBAAQA,WAAW,KAAnB,sCAAjO,kBAA0T,sBAAQA,WAAW,KAAnB,UAA1T,iJACA,4CACA,4KAA2J,0BAAYA,WAAW,KAAvB,2BAA3J,qBAAoP,mBAAGA,WAAW,KAAQ,CACtQ,KAAQ,yDADwO,8CAApP,WAEmE,mBAAGA,WAAW,KAAQ,CACrF,KAAQ,0DADuD,yDAFnE,KAKA,4BAAW,sBAAQA,WAAW,KAAnB,2BAAX,8PACA,8GAA6F,mBAAGA,WAAW,KAAQ,CAC/G,KAAQ,+EADiF,QAA7F,oLAGA,uKAAsJ,0BAAYA,WAAW,KAAvB,oBAAtJ,4FACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,ubAkBL,mDAAkC,sBAAQA,WAAW,KAAnB,oBAAlC,aAAoG,sBAAQA,WAAW,KAAnB,cAApG,0CACA,oFACA,sBACE,kBAAIA,WAAW,MAAf,qBAA0C,sBAAQA,WAAW,MAAnB,gCAA1C,KACA,kBAAIA,WAAW,MAAK,sBAAQA,WAAW,MAAnB,uCAApB,0BACA,kBAAIA,WAAW,MAAK,sBAAQA,WAAW,MAAnB,mBAApB,6CAEF,sCACA,8MACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,8SAiBL,mRAAkQ,0BAAYA,WAAW,KAAvB,gCAAlQ,sDACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,glBAsCL,oJACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,yLAcL,oGAAmF,0BAAYA,WAAW,KAAvB,0BAAnF,YACA,6EAA4D,0BAAYA,WAAW,KAAvB,mBAA5D,mGACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,oBADZ,46BAoBL,0CACA,uNACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,kQAaL,qJAAoI,0BAAYA,WAAW,KAAvB,6BAApI,KACA,uBAAK,sBAAMA,WAAW,OAAU,IAA3B,6xCAgDL,+CACA,6LACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,mBADZ,uMAUL,yCAAwB,sBAAQA,WAAW,KAAnB,kCAAxB,4CACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,oBADZ,kEAIL,kCACA,sCAAqB,0BAAYA,WAAW,KAAvB,iBAArB,uCAAsH,sBAAQA,WAAW,KAAnB,gBAAtH,yHAAgS,0BAAYA,WAAW,KAAvB,mBAAhS,uCAAmY,sBAAQA,WAAW,KAAnB,gBAAnY,cAAkc,sBAAQA,WAAW,KAAnB,mBAAlc,8BAAohB,sBAAQA,WAAW,KAAnB,sCAAphB,uCACA,wCAAuB,0BAAYA,WAAW,KAAvB,gBAAvB,wDACA,8CAA6B,0BAAYA,WAAW,KAAvB,oBAC7B,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,oBADZ,imCA0BL,8EACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,oBADZ,gmBAYL,kCACA,oEAAmD,0BAAYA,WAAW,KAAvB,iBAAnD,sPAAmW,0BAAYA,WAAW,KAAvB,mBAAnW,uCAAsc,sBAAQA,WAAW,KAAnB,gBAAtc,kDACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,oBADZ,oyBAyBL,yCACA,kDAAiC,mBAAGA,WAAW,KAAQ,CACnD,KAAQ,8DADqB,6BAAjC,6CAEoF,mBAAGA,WAAW,KAAQ,CACtG,KAAQ,0DADwE,qBAFpF,KAKA,uOAAsN,mBAAGA,WAAW,KAAQ,CACxO,KAAQ,8GAD0M,8GAGtN,8HAA6G,mBAAGA,WAAW,KAAQ,CAC/H,KAAQ,0FADiG,0FAG7G,qCACA,8EAA6D,0BAAYA,WAAW,KAAvB,YAA7D,2HACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,4WAsBL,sBACE,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,WAA+B,sBAAQA,WAAW,KAAnB,YAA/B,sOACA,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,CAC5C,UAAa,kBADI,uFAOvB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,+BACA,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,CAC5C,UAAa,kBADI,iFAOvB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,2BACA,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,CAC5C,UAAa,kBADI,iZAYvB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,iDACA,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,CAC5C,UAAa,kBADI,8WAyBvB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAd,wDACA,mBAAKA,WAAW,MAAK,sBAAMA,WAAW,OAAU,CAC5C,UAAa,kBADI,oXA0BzB,wCACA,8dACA,uKAAsJ,mBAAGA,WAAW,KAAQ,CACxK,KAAQ,oFAD0I,kBAAtJ,OAEmC,mBAAGA,WAAW,KAAQ,CACrD,KAAQ,0EADuB,wBAFnC,8TAKA,6gBAA4f,mBAAGA,WAAW,KAAQ,CAC9gB,KAAQ,0EADgf,qBAA5f,8OAGA,4EACA,sBACE,kBAAIA,WAAW,MAAf,YACA,kBAAIA,WAAW,MAAf,WACA,kBAAIA,WAAW,MAAf,SAEF,gDACA,0DAAyC,sBAAQA,WAAW,KAAnB,8EAAzC,KACA,oOACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,ycA0BL,+GACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,oBADZ,moCAkBL,kZAAiY,0BAAYA,WAAW,KAAvB,kBAAjY,oKACA,8FACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,grBAcL,sGAAqF,kBAAIA,WAAW,KAAf,YAArF,yBAAmJ,sBAAQA,WAAW,KAAnB,WAAnJ,6HACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,+eA2BL,yHACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,usBAcL,oXACA,0HACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,gcAiBL,yTACA,gDAA+B,0BAAYA,WAAW,KAAvB,UAA/B,eACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,inBAaL,+CACA,0DAAyC,sBAAQA,WAAW,KAAnB,kFAAzC,KACA,mEAAkD,kBAAIA,WAAW,KAAf,WAAlD,iEACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,kLAOL,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,mFAML,8GAA6F,kBAAIA,WAAW,KAAf,WAA7F,+MAAgV,0BAAYA,WAAW,KAAvB,UAAhV,gDACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,knBAaL,wFAAuE,0BAAYA,WAAW,KAAvB,UAAvE,0HACA,6CAA4B,sBAAQA,WAAW,KAAnB,0EAA5B,OAA8I,0BAAYA,WAAW,KAAvB,UAA9I,iLAAkX,0BAAYA,WAAW,KAAvB,UAAlX,kDACA,6CACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,wuBAcL,qVAAoU,0BAAYA,WAAW,KAAvB,UAApU,4CACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,inBAaL,+EAA8D,0BAAYA,WAAW,KAAvB,UAA9D,sCACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,oBADZ,+KAOL,wVAAuU,0BAAYA,WAAW,KAAvB,QAAvU,gDACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,osBAcL,yJACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,oBADZ,kMAOL,uJACA,4CACA,iDAAgC,sBAAQA,WAAW,KAAnB,yDAAhC,oDAA8K,sBAAQA,WAAW,KAAnB,0GAA9K,KACA,qVACA,0DAAyC,0BAAYA,WAAW,KAAvB,kBAAzC,mFACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,wkBAaL,+HACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,oBADZ,sKAOL,8HAA6G,0BAAYA,WAAW,KAAvB,WAA7G,6CAA8M,0BAAYA,WAAW,KAAvB,SAC9M,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,kBADZ,wrBAcL,mGACA,uBAAK,sBAAMA,WAAW,OAAU,CAC5B,UAAa,oBADZ,qMAOL,kDACA,qLACA,oCACA,iHACA,sBACE,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAK,mBAAGA,WAAW,KAAQ,CACrC,KAAQ,mDADO,oDAIrB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAK,mBAAGA,WAAW,KAAQ,CACrC,KAAQ,0CADO,2CAIrB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAK,mBAAGA,WAAW,KAAQ,CACrC,KAAQ,iEADO,kEAIrB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAK,mBAAGA,WAAW,KAAQ,CACrC,KAAQ,wHADO,yHAIrB,kBAAIA,WAAW,MACb,iBAAGA,WAAW,MAAK,mBAAGA,WAAW,KAAQ,CACrC,KAAQ,6FADO,gGAS3BJ,EAAWK,gBAAiB","file":"component---src-pages-avro-avro-mdx-578c4fc66c02b2e5ff5b.js","sourcesContent":["import React from 'react'\n  /* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsx mdx */\n\nimport DefaultLayout from \"/home/runner/work/refarch-kc/refarch-kc/docs-gatsby/node_modules/gatsby-theme-carbon/src/templates/Default.js\";\nexport const _frontmatter = {};\n\nconst makeShortcode = name => function MDXDefaultShortcode(props) {\n  console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n  return <div {...props} />;\n};\n\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout;\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <h2>{`Introduction`}</h2>\n    <p>{`Here we explain the `}<a parentName=\"p\" {...{\n        \"href\": \"https://avro.apache.org/\"\n      }}>{`Apache Avro`}</a>{` messaging integration we have done in one of our integration tests for the `}<strong parentName=\"p\">{`refarch-kc-container-ms`}</strong>{` component, which is part of the `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm-cloud-architecture.github.io/refarch-kc/\"\n      }}>{`Reefer Containers reference implementation`}</a>{` of the `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm-cloud-architecture.github.io/refarch-eda/\"\n      }}>{`IBM Event Driven Architectures reference architecture`}</a>{`. The Reefer Containers reference implementation is a simulation of what a container shipment process could look like in reality. From a manufacturer creating some goods to the delivery of those to a retailer, going through requesting a container, loading the goods into the container, finding a voyage for that container on a ship, monitoring the container’s temperature and GPS location, delivering the container, unloading the goods, etc. As you can imagine, this scenario is ideal for an Event Driven architecture where we not only have a microservices based application but also the integration of these using Event Driven Architecture components (such as Kafka) and patterns (such as Saga, CQRS, Event Sourcing, etc).`}</p>\n    <h3>{`What is Apache Avro`}</h3>\n    <p>{`Avro is an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.`}</p>\n    <h3>{`Why Apache Avro`}</h3>\n    <p>{`There are several websites that discuss the Apache Avro data serialization system benefits over other messaging data protocols. A simple google search will list dozens of them. Here, we will highlight just a few from a `}<a parentName=\"p\" {...{\n        \"href\": \"https://www.confluent.io/blog/avro-kafka-data/\"\n      }}>{`Confluent blog post`}</a>{`:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`It has a direct mapping to and from JSON`}</li>\n      <li parentName=\"ul\">{`It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.`}</li>\n      <li parentName=\"ul\">{`It is very fast.`}</li>\n      <li parentName=\"ul\">{`It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.`}</li>\n      <li parentName=\"ul\">{`It has a rich, extensible schema language defined in pure JSON`}</li>\n      <li parentName=\"ul\">{`It has the best notion of compatibility for evolving your data over time.`}</li>\n    </ul>\n    <h2>{`Avro, Kafka and Schema Registry`}</h2>\n    <p>{`Avro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format. Schema Registry defines a scope in which schemas can evolve, and that scope is the subject. The name of the subject depends on the configured subject name strategy, which by default is set to derive subject name from topic name.`}</p>\n    <p>{`In our case, this Avro data are messages sent to a kafka topic. Each message is a key-value pair. Either the message key or the message value, or both, can be serialized as Avro. Integration with Schema Registry means that Kafka messages do not need to be written with the entire Avro schema. Instead, Kafka messages are written with the `}<strong parentName=\"p\">{`schema id`}</strong>{`. The producers writing the messages and the consumers reading the messages must be using the same Schema Registry to get the same mapping between a schema and schema id.`}</p>\n    <p>{`Kafka is used as Schema Registry storage backend. The special Kafka topic `}<inlineCode parentName=\"p\">{`<kafkastore.topic>`}</inlineCode>{` (default `}<inlineCode parentName=\"p\">{`_schemas`}</inlineCode>{`), with a single partition, is used as a highly available write ahead log. All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log. A Schema Registry instance therefore both produces and consumes messages under the `}<inlineCode parentName=\"p\">{`_schemas`}</inlineCode>{` topic. It produces messages to the log when, for example, new schemas are registered under a subject, or when updates to compatibility settings are registered. Schema Registry consumes from the `}<inlineCode parentName=\"p\">{`_schemas`}</inlineCode>{` log in a background thread, and updates its local caches on consumption of each new `}<inlineCode parentName=\"p\">{`_schemas`}</inlineCode>{` message to reflect the newly added schema or compatibility setting. Updating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability.`}</p>\n    <h3>{`How does it all work`}</h3>\n    <span {...{\n      \"className\": \"gatsby-resp-image-wrapper\",\n      \"style\": {\n        \"position\": \"relative\",\n        \"display\": \"block\",\n        \"marginLeft\": \"auto\",\n        \"marginRight\": \"auto\",\n        \"maxWidth\": \"1152px\"\n      }\n    }}>{`\n      `}<span parentName=\"span\" {...{\n        \"className\": \"gatsby-resp-image-background-image\",\n        \"style\": {\n          \"paddingBottom\": \"44.79166666666667%\",\n          \"position\": \"relative\",\n          \"bottom\": \"0\",\n          \"left\": \"0\",\n          \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB2klEQVQoz1VSYW8SQRC9n+8X037QpprohybVJiWCEmmotjQBi9pIDYm2BcVej0LhDg64vVt2b3efs8cV6yZvb3Yy92bmzTg99w9KlSYK71pofDmDNhqrY3L8f4yxMDke+k0W7Zydt7Cx/Qqb2yW8LhYRhQyjcYrOL427sUISCyyCBCzkkDzFbCbw81LhsqOwWAhwJiCX6TqJc9HpotaoExr4cPIJnC8RJwZXXQFvwLPsginERCSEJKQI5xqTqaZuVk2IWK0rdTjnGI7GGPkCcfyvTcYZPD9Cq93GwcdjXFx1M3+apmtJpmGIYBJgOBjB931YLkdIidl8jjCy1ei1HjLVBIXjegP75TK6vd6qWoo3OfFhrYZipYpy9QglinE9D869/K6n4AcKWt+TUktaZTC576GtlKZhVvB4q4BHm+/xbGcX1zc3ltBQNk2EknRRWXWKMki6yJ3ZVitrW5/FaroGw/EQv689/Oj04fb7SGzLJp99vJTghH6wxJPDAXZO53hx4mOreounBx6eH91ht7nAy/oM525CMkl4t/QfN4hoEyyPlSGr0D70asEQshR7zQnK3xnefpuh8DnA/ukYb75OUWkzlFoRejRAnmgiTMFiSVsRYxFF9E3wFy3zoehJrCOvAAAAAElFTkSuQmCC')\",\n          \"backgroundSize\": \"cover\",\n          \"display\": \"block\"\n        }\n      }}></span>{`\n  `}<img parentName=\"span\" {...{\n        \"className\": \"gatsby-resp-image-image\",\n        \"alt\": \"schema registry management\",\n        \"title\": \"schema registry management\",\n        \"src\": \"/refarch-kc/static/36bdff6a4e40d5c09d4ab48404d08928/3cbba/schema-registry-and-kafka.png\",\n        \"srcSet\": [\"/refarch-kc/static/36bdff6a4e40d5c09d4ab48404d08928/7fc1e/schema-registry-and-kafka.png 288w\", \"/refarch-kc/static/36bdff6a4e40d5c09d4ab48404d08928/a5df1/schema-registry-and-kafka.png 576w\", \"/refarch-kc/static/36bdff6a4e40d5c09d4ab48404d08928/3cbba/schema-registry-and-kafka.png 1152w\", \"/refarch-kc/static/36bdff6a4e40d5c09d4ab48404d08928/dc66d/schema-registry-and-kafka.png 1598w\"],\n        \"sizes\": \"(max-width: 1152px) 100vw, 1152px\",\n        \"style\": {\n          \"width\": \"100%\",\n          \"height\": \"100%\",\n          \"margin\": \"0\",\n          \"verticalAlign\": \"middle\",\n          \"position\": \"absolute\",\n          \"top\": \"0\",\n          \"left\": \"0\"\n        },\n        \"loading\": \"lazy\"\n      }}></img>{`\n    `}</span>\n    <p>{`When the producer sends a message/event to a Kafka topic for the first time, it sends the schema for that message/event to the Schema Registry. The Schema Registry registers this schema to the subject for the Kafka topic we want to send the message/event to, and returns the `}<strong parentName=\"p\">{`schema id`}</strong>{` to the producer. The producer `}<strong parentName=\"p\">{`caches this mapping between the schema and schema id`}</strong>{` for subsequent message writes, so `}<strong parentName=\"p\">{`it only contacts Schema Registry on the first message/event write`}</strong>{` (unless the schema has changed, that is evolved, when the schema registry will be contacted again for validation and storage of this new version of the schema). Kafka messages are written along with the `}<strong parentName=\"p\">{`schema id`}</strong>{` rather than with the entire data schema.`}</p>\n    <p>{`When a consumer reads this data, it sees the `}<strong parentName=\"p\">{`Avro schema id and sends a schema request to Schema Registry`}</strong>{`. Schema Registry retrieves the schema associated to that schema id, and `}<strong parentName=\"p\">{`returns the schema to the consumer`}</strong>{`. The consumer `}<strong parentName=\"p\">{`caches`}</strong>{` this mapping between the schema and schema id for subsequent message reads, so it only contacts Schema Registry on the first schema id read.`}</p>\n    <h2>{`Our implementation`}</h2>\n    <p>{`As mentioned in the introduction, the integration of the Apache Avro data serialization system has been done in one of the integration test for the `}<inlineCode parentName=\"p\">{`refarch-kc-container-ms`}</inlineCode>{` component of the `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm-cloud-architecture.github.io/refarch-kc/\"\n      }}>{`Reefer Containers reference implementation`}</a>{` of the `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm-cloud-architecture.github.io/refarch-eda/\"\n      }}>{`IBM Event Driven Architectures reference architecture`}</a>{`.`}</p>\n    <p>{`The `}<strong parentName=\"p\">{`refarch-kc-container-ms`}</strong>{` component will take care of the reefer containers status. From adding new reefers to the available containers list to assigning a container to a particular order and managing the status of that reefer throughout the shipment process aforementioned. `}</p>\n    <p>{`The integration tests for our Reefer Containers reference implementation can be found `}<a parentName=\"p\" {...{\n        \"href\": \"https://github.com/ibm-cloud-architecture/refarch-kc/tree/master/itg-tests\"\n      }}>{`here`}</a>{`. The integration tests are being developed in python and their main goal is to validate the successful deployment of the Reefer Containers reference implementation end-to-end.`}</p>\n    <p>{`The particular integration test (still under development) where we have integrated the Apache Avro serialization system can be found under the `}<inlineCode parentName=\"p\">{`ContainersPython`}</inlineCode>{` folder. More precisely, these are the files and folders involved in our implementation:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`├── data_schemas/avro_test\n│   ├── container_event.avsc\n│   ├── container_event_key.avsc\n│   ├── container_event_payload.avsc\n│   ├── container_event_type.avsc\n│   ├── container_status.avsc\n│   └── utils\n│       └── avroEDAUtils.py\n└── itg-tests\n    ├── ContainersPython\n    │   ├── ConsumeAvroContainers.py\n    │   └── ContainerAvroProducer.py\n    └── kafka\n        ├── KcAvroConsumer.py\n        └── KcAvroProducer.py\n`}</code></pre>\n    <p>{`that will allow us to send `}<strong parentName=\"p\">{`container events`}</strong>{` into the `}<strong parentName=\"p\">{`containers`}</strong>{` Kafka topic and read from such topic.`}</p>\n    <p>{`By using these python scripts, we will be able to validate:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Sending/Receiving `}<strong parentName=\"li\">{`Apache Avro encoded messages`}</strong>{`.`}</li>\n      <li parentName=\"ul\"><strong parentName=\"li\">{`Apache Avro data schema definitions`}</strong>{` for data correctness.`}</li>\n      <li parentName=\"ul\"><strong parentName=\"li\">{`Schema Registry`}</strong>{` for Apache Avro data schema management.`}</li>\n    </ul>\n    <h3>{`Data Schemas`}</h3>\n    <p>{`Avro schemas are defined with JSON. An example of a Container Event for creating a new reefer container to the available list of containers for our reference application looks like:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-json\"\n      }}>{`{\n  \"containerID\": \"container01\",\n  \"timestamp\": 1569410690,\n  \"type\": \"ContainerAdded\",\n  \"payload\": {\n    \"containerID\": \"container01\",\n    \"type\": \"Reefer\",\n    \"status\": \"Empty\",\n    \"latitude\": 37.8,\n    \"longitude\": -122.25,\n    \"capacity\": 110,\n    \"brand\": \"itg-brand\"\n  }\n}\n`}</code></pre>\n    <p>{`An Avro schema could be a nested schema which allows us to have a smaller reusable data schemas to define bigger and more complex ones. This is the case for our Container Event data schema. For instance, the payload is defined on its own data schema (`}<inlineCode parentName=\"p\">{`container_event_payload.avsc`}</inlineCode>{`) which the Container Event data schema refers to:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-json\"\n      }}>{`{\n  \"namespace\": \"ibm.eda.kc.container.event\",\n  \"name\": \"payload\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"containerID\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"type\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"status\",\n      \"type\": \"ibm.eda.kc.container.status\"\n    },\n    {\n      \"name\": \"latitude\",\n      \"type\": \"float\"\n    },\n    {\n      \"name\": \"longitude\",\n      \"type\": \"float\"\n    },\n    {\n      \"name\": \"capacity\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"brand\",\n      \"type\": \"string\"\n    }\n  ]\n}\n`}</code></pre>\n    <p>{`As you can see, the status attribute of the payload is yet another data schema itself which, in this case, is of type enum:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-json\"\n      }}>{`{\n  \"namespace\": \"ibm.eda.kc.container\",\n  \"name\": \"status\",\n  \"type\": \"enum\",\n  \"symbols\": [\n    \"Loaded\",\n    \"Empty\",\n    \"Unassignable\",\n    \"PartiallyLoaded\"\n  ]\n}\n`}</code></pre>\n    <p>{`All the different data schemas for a Container Event can be found under the `}<inlineCode parentName=\"p\">{`data_schemas/avro_test`}</inlineCode>{` folder.`}</p>\n    <p>{`In that folder we have also developed a util script (`}<inlineCode parentName=\"p\">{`avroEDAUtils.py`}</inlineCode>{`) to be able to construct the final Container Event data schema that is needed by our producer:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`def getContainerEventSchema(schema_files_location):\n  # Read all the schemas needed in order to produce the final Container Event Schema\n  known_schemas = avro.schema.Names()\n  container_status_schema = LoadAvsc(schema_files_location + \"container_status.avsc\", known_schemas)\n  container_event_payload_schema = LoadAvsc(schema_files_location + \"container_event_payload.avsc\", known_schemas)\n  container_event_type_schema = LoadAvsc(schema_files_location + \"container_event_type.avsc\", known_schemas)\n  container_event_schema = LoadAvsc(schema_files_location + \"container_event.avsc\", known_schemas)\n  return container_event_schema\n\ndef LoadAvsc(file_path, names=None):\n  # Load avsc file\n  # file_path: path to schema file\n  # names(optional): avro.schema.Names object\n  file_text = open(file_path).read()\n  json_data = json.loads(file_text)\n  schema = avro.schema.SchemaFromJSONData(json_data, names)\n  return schema\n`}</code></pre>\n    <h4>{`See it in action`}</h4>\n    <p>{`Here, we are going to see how data schemas help with data correctness. Using the payload for our container messages/events as the example, this is the output of a correct message being sent:`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`--- Container event to be published: ---\n{\n  \"containerID\": \"container01\",\n  \"type\": \"Reefer\",\n  \"status\": \"Empty\",\n  \"latitude\": 37.8,\n  \"longitude\": -122.25,\n  \"capacity\": 110,\n  \"brand\": \"itg-brand\"\n}\n\nMessage delivered to containers [0]\n`}</code></pre>\n    <p>{`However, if we try to send a payload where, for instance the container ID is an integer rather than a string, we will get an `}<inlineCode parentName=\"p\">{`avro.io.AvroTypeException`}</inlineCode>{`:`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`avro.io.AvroTypeException: The datum {'containerID': 12345, 'type': 'Reefer', 'status': 'Empty', 'latitude': 37.8, 'longitude': -122.25, 'capacity': 110, 'brand': 'itg-brand'} is not an example of the schema\n{\n    \"type\": {\n        \"type\": \"record\",\n        \"name\": \"payload\",\n        \"namespace\": \"ibm.eda.kc.container.event\",\n        \"fields\": [\n          {\n            \"type\": \"string\",\n            \"name\": \"containerID\"\n          },\n          {\n            \"type\": \"string\",\n            \"name\": \"type\"\n          },\n          {\n            \"type\": {\n              \"type\": \"enum\",\n              \"name\": \"status\",\n              \"namespace\": \"ibm.eda.kc.container\",\n              \"symbols\": [\n                \"Loaded\",\n                \"Empty\",\n                \"Unassignable\",\n                \"PartiallyLoaded\"\n              ]\n            },\n            \"name\": \"status\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"latitude\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"longitude\"\n          },\n          {\n            \"type\": \"int\",\n            \"name\": \"capacity\"\n          },\n          {\n            \"type\": \"string\",\n            \"name\": \"brand\"\n          }\n        ]\n      }\n`}</code></pre>\n    <h3>{`Producer and Consumer`}</h3>\n    <p>{`The python scripts developed to implement a producer and consumer to a kafka topic that sends Avro messages whose data schemas are managed by a schema registry are:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`└── itg-tests\n    ├── ContainersPython\n    │   ├── ConsumeAvroContainers.py\n    │   └── ContainerAvroProducer.py\n    └── kafka\n        ├── KcAvroConsumer.py\n        └── KcAvroProducer.py\n`}</code></pre>\n    <p>{`We have used the `}<strong parentName=\"p\">{`confluent_kafka avro libraries`}</strong>{` to implement our producer and consumer.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`from confluent_kafka.avro import AvroProducer, AvroConsumer\n`}</code></pre>\n    <h4>{`Producer`}</h4>\n    <p>{`We create our `}<inlineCode parentName=\"p\">{`KafkaProducer`}</inlineCode>{` object where we define some of the `}<strong parentName=\"p\">{`AvroProducer`}</strong>{` options such as the schema registry url for data schema registration and management. But it is not until we call the `}<inlineCode parentName=\"p\">{`prepareProducer`}</inlineCode>{` method that we actually create the `}<strong parentName=\"p\">{`AvroProducer`}</strong>{` with that `}<strong parentName=\"p\">{`schema registry`}</strong>{` to be used as well as the `}<strong parentName=\"p\">{`data schemas for the key and value`}</strong>{` of our Container Event to be sent.`}</p>\n    <p>{`Finally, in the `}<inlineCode parentName=\"p\">{`publishEvent`}</inlineCode>{` method we send a value plus a key to a kafka topic.`}</p>\n    <p>{`producer when we call `}<inlineCode parentName=\"p\">{`prepareProducer`}</inlineCode></p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`import json\nfrom confluent_kafka import KafkaError\nfrom confluent_kafka.avro import AvroProducer\n\nclass KafkaProducer:\n\n    def __init__(self,kafka_brokers = \"\",kafka_apikey = \"\",schema_registry_url = \"\"):\n        self.kafka_brokers = kafka_brokers\n        self.kafka_apikey = kafka_apikey\n        self.schema_registry_url = schema_registry_url\n\n    def prepareProducer(self,groupID = \"pythonproducers\",key_schema = \"\", value_schema = \"\"):\n        options ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'schema.registry.url': self.schema_registry_url,\n                'group.id': groupID\n        }\n        self.producer = AvroProducer(options,default_key_schema=key_schema,default_value_schema=value_schema)\n\n    def publishEvent(self, topicName, value, key):\n        # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first\n        self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(value)[key], callback=self.delivery_report)\n        self.producer.flush()\n`}</code></pre>\n    <p>{`To use this class you need to do the following steps:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`# load schema definitions for key and value\nfrom utils.avroEDAUtils import getContainerEventSchema, getContainerKeySchema\ncontainer_event_value_schema = getContainerEventSchema(\"/data_schemas/avro_test/\")\ncontainer_event_key_schema = getContainerKeySchema(\"/data_schemas/avro_test/\")\n# Create a producer with the schema registry URL end point\nkp = KafkaProducer(KAFKA_ENV,KAFKA_BROKERS,KAFKA_APIKEY,SCHEMA_REGISTRY_URL)\nkp.prepareProducer(\"ContainerProducerPython\",container_event_key_schema,container_event_value_schema)\n# loop on publishing events\nkp.publishEvent(TOPIC_NAME,container_event)\n`}</code></pre>\n    <h4>{`Consumer`}</h4>\n    <p>{`Similarly to the producer, when we create a `}<inlineCode parentName=\"p\">{`KafkaConsumer`}</inlineCode>{` object we are just setting some of its attributes such as the kafka topic we will listen to and the schema registry url the producer will retrieve the data schemas from based on the schema ids messages comes with. It is only when we call the `}<inlineCode parentName=\"p\">{`prepareConsumer`}</inlineCode>{` method that we actually create the `}<strong parentName=\"p\">{`AvroConsumer`}</strong>{` and subscribe it to the intended kafka topic.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`import json\nfrom confluent_kafka.avro import AvroConsumer\n\nclass KafkaConsumer:\n\n    def __init__(self, kafka_brokers = \"\", kafka_apikey = \"\", topic_name = \"\", schema_registry_url = \"\"):\n        self.kafka_brokers = kafka_brokers\n        self.kafka_apikey = kafka_apikey\n        self.topic_name = topic_name\n        self.schema_registry_url = schema_registry_url \n\n    def prepareConsumer(self, groupID = \"pythonconsumers\"):\n        options ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'group.id': groupID,\n                'auto.offset.reset': 'earliest',\n                'schema.registry.url': self.schema_registry_url,\n        }\n        self.consumer = AvroConsumer(options)\n        self.consumer.subscribe([self.topic_name])\n\n# ...\n`}</code></pre>\n    <h3>{`Schema registry`}</h3>\n    <p>{`For now, we have used the `}<a parentName=\"p\" {...{\n        \"href\": \"https://hub.docker.com/r/confluentinc/cp-schema-registry/\"\n      }}>{`Confluent schema registry`}</a>{` for our work although our goal is to use `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm.github.io/event-streams/schemas/overview/\"\n      }}>{`IBM Event Streams`}</a>{`.`}</p>\n    <p>{`The integration of the schema registry with your kafka broker is quite easy. In fact, all you need is to provide the schema registry with your zookeeper cluster url and give your schema registry a hostname: `}<a parentName=\"p\" {...{\n        \"href\": \"https://docs.confluent.io/current/installation/docker/config-reference.html#schema-registry-configuration\"\n      }}>{`https://docs.confluent.io/current/installation/docker/config-reference.html#schema-registry-configuration`}</a></p>\n    <p>{`Once you have your schema registry up and running, this provides a rich API endpoint to operate with: `}<a parentName=\"p\" {...{\n        \"href\": \"https://docs.confluent.io/current/schema-registry/using.html#common-sr-usage-examples\"\n      }}>{`https://docs.confluent.io/current/schema-registry/using.html#common-sr-usage-examples`}</a></p>\n    <p>{`For example:`}</p>\n    <p>{`Let’s assume we have created a new kafka topic called `}<inlineCode parentName=\"p\">{`avrotest`}</inlineCode>{` for testing our work. And let’s also assume we are sending persona messages/events whose data schema is the following:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-json\"\n      }}>{`{\n    \"namespace\": \"avro.test\",\n    \"name\": \"persona\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        }\n    ]\n }\n`}</code></pre>\n    <ul>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Get the `}<strong parentName=\"p\">{`subjects`}</strong>{` (that is, the kafka topics to which we have a schema registered against. As explained before, we either have registered the schema manually ourselves or the Avro producer has registered it when we have sent the first message)`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-http\"\n          }}>{`curl -X GET http://localhost:8081/subjects\n\n[\"avrotest-value\",\"avrotest-key\"]\n`}</code></pre>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Get versions for a subject:`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-http\"\n          }}>{`curl -X GET http://localhost:8081/subjects/avrotest-value/versions\n\n[1]\n`}</code></pre>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Get a specific version:`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-http\"\n          }}>{`curl -X GET http://localhost:8081/subjects/avrotest-value/versions/1/\n\n{\n  \"subject\": \"avrotest-value\",\n  \"version\": 1,\n  \"id\": 1,\n  \"schema\": \"{\\\\\"type\\\\\":\\\\\"record\\\\\",\\\\\"name\\\\\":\\\\\"persona\\\\\",\\\\\"namespace\\\\\":\\\\\"avro.test\\\\\",\\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"name\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},{\\\\\"name\\\\\":\\\\\"age\\\\\",\\\\\"type\\\\\":\\\\\"int\\\\\"},{\\\\\"name\\\\\":\\\\\"gender\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"}]}\"\n}\n`}</code></pre>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Get the schema of a specific subject version:`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-http\"\n          }}>{`curl -X GET http://localhost:8081/subjects/avrotest-value/versions/1/schema\n\n{\n  \"type\": \"record\",\n  \"name\": \"persona\",\n  \"namespace\": \"avro.test\",\n  \"fields\": [\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"age\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"gender\",\n      \"type\": \"string\"\n    }\n  ]\n}\n`}</code></pre>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\">{`Get the schema of a specific subject latest version:`}</p>\n        <pre parentName=\"li\"><code parentName=\"pre\" {...{\n            \"className\": \"language-http\"\n          }}>{`curl -X GET http://localhost:8081/subjects/avrotest-value/versions/latest/schema\n\n{\n  \"type\": \"record\",\n  \"name\": \"persona\",\n  \"namespace\": \"avro.test\",\n  \"fields\": [\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"age\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"gender\",\n      \"type\": \"string\"\n    }\n  ]\n}\n`}</code></pre>\n      </li>\n    </ul>\n    <h3>{`Data evolution`}</h3>\n    <p>{`So far we  have seen what Avro is, what a data schema is, what a schema registry is and how this all works together. From creating a data schema for your messages/events to comply with to how the schema registry and data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their schemas to the rich API the Confluent schema registry provides to interact with.`}</p>\n    <p>{`However, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm-cloud-architecture.github.io/refarch-eda/methodology/eventstorming/\"\n      }}>{`Event Storming`}</a>{` or `}<a parentName=\"p\" {...{\n        \"href\": \"https://ibm-cloud-architecture.github.io/refarch-eda/methodology/ddd/\"\n      }}>{`Domain Driven Design`}</a>{` for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data like your use or business cases may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases.`}</p>\n    <p>{`But it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event broker) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the event source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to `}<a parentName=\"p\" {...{\n        \"href\": \"https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/\"\n      }}>{`hundreds of years`}</a>{`) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, data at the end of the day.`}</p>\n    <p>{`There are mainly three types of data compatibility:`}</p>\n    <ol>\n      <li parentName=\"ol\">{`Backward`}</li>\n      <li parentName=\"ol\">{`Forward`}</li>\n      <li parentName=\"ol\">{`Full`}</li>\n    </ol>\n    <h4>{`Backward compatibility`}</h4>\n    <p>{`Backward compatibility means that `}<strong parentName=\"p\">{`consumers using the new schema can read data produced with the last schema`}</strong>{`.`}</p>\n    <p>{`Using the persona data schema already mentioned throughout this readme, what if we decide to change the data schema to add a new attribute such as place of birth? That is, the new schema would look like:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-json\"\n      }}>{`{\n    \"namespace\": \"avro.test\",\n    \"name\": \"persona\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"place_of_birth\",\n            \"type\" : \"string\"\n        }\n    ]\n}\n`}</code></pre>\n    <p>{`here is the output when we try to produce an event/message with the above data schema:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`### Persona event to be published: ###\n{'name': 'david', 'age': '25', 'gender': 'male', 'place_of_birth': 'USA'}\n######################################\nTraceback (most recent call last):\n  File \"ContainerAvroProducer.py\", line 73, in <module>\n    kp.publishEvent(TOPIC_NAME,container_event,\"1\")\n  File \"/home/kafka/KcAvroProducer.py\", line 42, in publishEvent\n    self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(key), callback=self.delivery_report)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\", line 80, in produce\n    value = self._serializer.encode_record_with_schema(topic, value_schema, value)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 105, in encode_record_with_schema\n    schema_id = self.registry_client.register(subject, schema)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/cached_schema_registry_client.py\", line 223, in register\n    raise ClientError(\"Invalid Avro schema:\" + str(code))\nconfluent_kafka.avro.error.ClientError: Invalid Avro schema:422\n`}</code></pre>\n    <p>{`And the reason for such error is that, because new schemas must be backward compatible (default compatibility mode for Confluent kafka data schemas topics), we can’t just simply add a new attribute. Consumers using the new schema must be able to read data produced with the last schema. That is, if a consumer was to read old messages with the schema above, it would expect the `}<inlineCode parentName=\"p\">{`place_of_birth`}</inlineCode>{` attribute and its value on these old messages. However, the messages were produced with the old schema that did not enforce such attribute. Hence, the problem.`}</p>\n    <p>{`We can also check the compatibility of this new schema using the API:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\\\"type\\\\\":\\\\\"record\\\\\",\n                     \\\\\"name\\\\\":\\\\\"persona\\\\\",\n                     \\\\\"namespace\\\\\":\\\\\"avro.test\\\\\",\n                     \\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"name\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"age\\\\\",\\\\\"type\\\\\":\\\\\"int\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"gender\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"place_of_birth\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"}]}\"}' \n        http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":false}\n`}</code></pre>\n    <p>{`How do we evolve our schema to add new attributes in a way that the schema is `}<em parentName=\"p\">{`BACKWARD`}</em>{` compatible? Adding a `}<strong parentName=\"p\">{`default`}</strong>{` value for such attribute so the consumer can use it when reading old messages that were produced without that attribute:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-json\"\n      }}>{`{\n    \"namespace\": \"avro.test\",\n    \"name\": \"persona\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"place_of_birth\",\n            \"type\" : \"string\",\n            \"default\": \"nonDefined\"\n        }\n    ]\n}\n`}</code></pre>\n    <p>{`Rather than changing it straight in the code, we can do some sort of validation through the API:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" \n--data '{\"schema\": \"{\\\\\"type\\\\\":\\\\\"record\\\\\",\n                     \\\\\"name\\\\\":\\\\\"persona\\\\\",\n                     \\\\\"namespace\\\\\":\\\\\"avro.test\\\\\",\n                     \\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"name\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"age\\\\\",\\\\\"type\\\\\":\\\\\"int\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"gender\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"place_of_birth\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\", \\\\\"default\\\\\":\\\\\"nonDefined\\\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n`}</code></pre>\n    <p>{`We now can evolve our data schema to enforce a new attribute with our new messages/events being produced but making sure the consumer is able to read old messages that do not contain such attribute. We do so by sending a new persona event/message along with this new data schema. This will make the schema registry to register the new data schema.`}</p>\n    <p>{`We can validate the new data schema version has been registered by using the schema registry API:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`\ncurl -X GET http://localhost:8081/subjects/avrotest-value/versions\n\n[1,2]\n\ncurl -X GET http://localhost:8081/subjects/avrotest-value/versions/latest/schema\n\n{\"type\":\"record\",\n \"name\":\"persona\",\n \"namespace\":\"avro.test\",\n \"fields\":[{\"name\":\"name\",\"type\":\"string\"},\n           {\"name\":\"age\",\"type\":\"int\"},\n           {\"name\":\"gender\",\"type\":\"string\"},\n           {\"name\":\"place_of_birth\",\"type\":\"string\",\"default\":\"nonDefined\"}]}\n`}</code></pre>\n    <p>{`What if we want to remove an attribute from our persona events/messages now? Well, this one is easy since the Avro consumer will simply ignore/drop all those attributes in the old persona events/messages that are not defined in the new data schema and just take in those that are defined.`}</p>\n    <p>{`Let’s try to remove the `}<inlineCode parentName=\"p\">{`gender`}</inlineCode>{` attribute:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\\\"type\\\\\":\\\\\"record\\\\\",\n                     \\\\\"name\\\\\":\\\\\"persona\\\\\",\n                     \\\\\"namespace\\\\\":\\\\\"avro.test\\\\\",\n                     \\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"name\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"age\\\\\",\\\\\"type\\\\\":\\\\\"int\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"place_of_birth\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\", \\\\\"default\\\\\":\\\\\"nonDefined\\\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n`}</code></pre>\n    <h4>{`Forward compatibility`}</h4>\n    <p>{`Forwards compatibility means that `}<strong parentName=\"p\">{`data produced with a new schema can be read by consumers using the last schema`}</strong>{`.`}</p>\n    <p>{`First, let’s set the compatibility type to `}<em parentName=\"p\">{`FORWARD`}</em>{` (default compatibility mode in Confluent kafka is backward):`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X PUT -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"compatibility\": \"FORWARD\"}' http://localhost:8081/config\n\n{\"compatibility\":\"FORWARD\"}\n\n`}</code></pre>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X GET http://localhost:8081/config\n\n{\"compatibilityLevel\":\"FORWARD\"}\n`}</code></pre>\n    <p>{`Now, how about removing an attribute when the compatibility type configured is set to `}<em parentName=\"p\">{`FORWARD`}</em>{`? In this case, it is not as simple as removing the attribute from the new schema as the consumer will expect such attribute that the producer will not add to the events/messages. Let’s try to remove the `}<inlineCode parentName=\"p\">{`gender`}</inlineCode>{` attribute from the persona messages/events:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\\\"type\\\\\":\\\\\"record\\\\\",\n                     \\\\\"name\\\\\":\\\\\"persona\\\\\",\n                     \\\\\"namespace\\\\\":\\\\\"avro.test\\\\\",\n                     \\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"name\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"age\\\\\",\\\\\"type\\\\\":\\\\\"int\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"place_of_birth\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\", \\\\\"default\\\\\":\\\\\"nonDefined\\\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":false}\n`}</code></pre>\n    <p>{`So, how can we produce new persona events/messages (without the `}<inlineCode parentName=\"p\">{`gender`}</inlineCode>{` attribute) that are compatible with the last data schema used by consumers (that expects an attribute called gender)?`}</p>\n    <p>{`The trick here is to `}<strong parentName=\"p\">{`first register an “intermediate” data schema that adds a default value`}</strong>{` to `}<inlineCode parentName=\"p\">{`gender`}</inlineCode>{` if it is not defined. This way, the “intermediate” data schema will become the last data schema for the consumers and when we producer sent messages that do not contain the `}<inlineCode parentName=\"p\">{`gender`}</inlineCode>{` attribute, the consumer will know what to do:`}</p>\n    <p>{`Intermediate schema:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\\\"type\\\\\":\\\\\"record\\\\\",\n                     \\\\\"name\\\\\":\\\\\"persona\\\\\",\n                     \\\\\"namespace\\\\\":\\\\\"avro.test\\\\\",\n                     \\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"name\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"age\\\\\",\\\\\"type\\\\\":\\\\\"int\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"gender\\\\\",\\\\\"type\\\\\": \\\\\"string\\\\\",\\\\\"default\\\\\": \\\\\"nonProvided\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"place_of_birth\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\", \\\\\"default\\\\\":\\\\\"nonDefined\\\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n`}</code></pre>\n    <p>{`We register this data schema either by sending it along with a message/event using our producer or we simply register it using the schema registry API. Once we have this “intermediate” schema registered that will actually become the last data schema for the consumer, we check if our end goal data schema without the `}<inlineCode parentName=\"p\">{`gender`}</inlineCode>{` attribute is forward compatible or not:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\\\"type\\\\\":\\\\\"record\\\\\",\n                     \\\\\"name\\\\\":\\\\\"persona\\\\\",\n                     \\\\\"namespace\\\\\":\\\\\"avro.test\\\\\",\n                     \\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"name\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"age\\\\\",\\\\\"type\\\\\":\\\\\"int\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"place_of_birth\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\", \\\\\"default\\\\\":\\\\\"nonDefined\\\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n`}</code></pre>\n    <p>{`If we send a persona message that does not contain the `}<inlineCode parentName=\"p\">{`gender`}</inlineCode>{` attribute now, we should succeed:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`### Persona event to be published: ###\n{'name': 'david', 'age': 25, 'place_of_birth': 'USA'}\n######################################\nMessage delivered to avrotest [0]\n`}</code></pre>\n    <p>{`Contrary to the backward compatibility, in forward compatibility, adding a new attribute to your events/messages is not a problem because the consumers will simply ignore/drop this new attribute since the schema they are still using (the last one) does not include it. So let’s say we want to add a new attribute called `}<inlineCode parentName=\"p\">{`hair`}</inlineCode>{` to represent the color of a persona’s hair:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\\\"type\\\\\":\\\\\"record\\\\\",\n                     \\\\\"name\\\\\":\\\\\"persona\\\\\",\n                     \\\\\"namespace\\\\\":\\\\\"avro.test\\\\\",\n                     \\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"name\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"age\\\\\",\\\\\"type\\\\\":\\\\\"int\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"place_of_birth\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\", \\\\\"default\\\\\":\\\\\"nonDefined\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"hair\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n`}</code></pre>\n    <p>{`We see there is no problem at all and if we try to send a message/event containing this new attribute along with the new schema:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`### Persona event to be published: ###\n{'name': 'John', 'age': 25, 'place_of_birth': 'London', 'hair': 'brown'}\n######################################\nMessage delivered to avrotest [0]\n`}</code></pre>\n    <p>{`The new data schema is registered and new messages/events complying with that new data schema are sent with no problem at all.`}</p>\n    <h4>{`Full compatibility`}</h4>\n    <p>{`Full compatibility means `}<strong parentName=\"p\">{`data schemas are both backward and forward compatible`}</strong>{`. Data schemas evolve in a fully compatible way: `}<strong parentName=\"p\">{`old data can be read with the new data schema, and new data can also be read with the last data schema`}</strong>{`.`}</p>\n    <p>{`In some data formats, such as JSON, there are no full-compatible changes. Every modification is either only forward or only backward compatible. But in other data formats, like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change.`}</p>\n    <p>{`So let’s see if we can delete the `}<inlineCode parentName=\"p\">{`place_of_birth`}</inlineCode>{` attribute, the only attribute in our data schema that defines a default value:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\\\"type\\\\\":\\\\\"record\\\\\",\n                     \\\\\"name\\\\\":\\\\\"persona\\\\\",\n                     \\\\\"namespace\\\\\":\\\\\"avro.test\\\\\",\n                     \\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"name\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"age\\\\\",\\\\\"type\\\\\":\\\\\"int\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"hair\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"}]}\"}' \nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n`}</code></pre>\n    <p>{`It looks like it may work. Let’s send a message without that attribute along with the new data schema:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`### Persona event to be published: ###\n{'name': 'John', 'age': 25, 'hair': 'brown'}\n######################################\nMessage delivered to avrotest [0]\n`}</code></pre>\n    <p>{`Let’s now try to add an attribute with a default value. Let’s say we want to add an attribute for the `}<inlineCode parentName=\"p\">{`hobbies`}</inlineCode>{` of a persona whose default value will be `}<inlineCode parentName=\"p\">{`none`}</inlineCode></p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-http\"\n      }}>{`curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\\\"type\\\\\":\\\\\"record\\\\\",\n                     \\\\\"name\\\\\":\\\\\"persona\\\\\",\n                     \\\\\"namespace\\\\\":\\\\\"avro.test\\\\\",\n                     \\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"name\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"age\\\\\",\\\\\"type\\\\\":\\\\\"int\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"hair\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\"},\n                                 {\\\\\"name\\\\\":\\\\\"hobbies\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\", \\\\\"default\\\\\":\\\\\"none\\\\\"}]}\"}'\nßhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n`}</code></pre>\n    <p>{`Let’s send a message along with the new data schema to be completely sure:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`### Persona event to be published: ###\n{'name': 'John', 'age': 25, 'hair': 'brown', 'hobbies': 'dance,music,food'}\n######################################\nMessage delivered to avrotest [0]\n`}</code></pre>\n    <p>{`As expected, it did work.`}</p>\n    <p>{`We now know how a data schema can evolve when full compatibility is required. That is, we know what attributes can be removed and how to add new attributes.`}</p>\n    <h2>{`Compendium`}</h2>\n    <p>{`Here are some links we have visited to carry out our work and found interesting to read:`}</p>\n    <ul>\n      <li parentName=\"ul\">\n        <p parentName=\"li\"><a parentName=\"p\" {...{\n            \"href\": \"https://www.confluent.io/blog/avro-kafka-data/\"\n          }}>{`https://www.confluent.io/blog/avro-kafka-data/`}</a></p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\"><a parentName=\"p\" {...{\n            \"href\": \"https://avro.apache.org/docs/current/\"\n          }}>{`https://avro.apache.org/docs/current/`}</a></p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\"><a parentName=\"p\" {...{\n            \"href\": \"https://docs.confluent.io/current/schema-registry/index.html\"\n          }}>{`https://docs.confluent.io/current/schema-registry/index.html`}</a></p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\"><a parentName=\"p\" {...{\n            \"href\": \"https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html#schema-registry-tutorial-definition\"\n          }}>{`https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html#schema-registry-tutorial-definition`}</a></p>\n      </li>\n      <li parentName=\"ul\">\n        <p parentName=\"li\"><a parentName=\"p\" {...{\n            \"href\": \"https://docs.oracle.com/database/nosql-11.2.2.0/GettingStartedGuide/schemaevolution.html\"\n          }}>{`https://docs.oracle.com/database/nosql-11.2.2.0/GettingStartedGuide/schemaevolution.html`}</a></p>\n      </li>\n    </ul>\n\n    </MDXLayout>;\n}\n;\nMDXContent.isMDXComponent = true;\n      "],"sourceRoot":""}