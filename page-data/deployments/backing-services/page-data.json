{"componentChunkName":"component---src-pages-deployments-backing-services-mdx","path":"/deployments/backing-services/","result":{"pageContext":{"frontmatter":{"title":"Deployment of backing services","description":"Deployment of backing services"},"relativePagePath":"/deployments/backing-services.mdx","titleType":"append","MdxNode":{"id":"3f38a812-4dd4-5926-ba6c-a73509a200a2","children":[],"parent":"32ca696a-74ec-516b-979a-147c5b86e97c","internal":{"content":"---\ntitle: Deployment of backing services\ndescription: Deployment of backing services\n---\n\n\n## Kafka & Event Streams\n\n### Using IBM Event Streams, hosted on IBM Cloud\n\n#### Services Deployment\n\nWe recommend to follow [our most recent lab](https://ibm-cloud-architecture.github.io/refarch-eda/technology/event-streams/es-cloud/) on how to provision an Event Streams intance on cloud.\n\n* In the *Manage* panel add the topics needed for the solution. We need at least the following:\n\n ![](images/IES-IC-topics.png)\n\n* In the Service Credentials tab, create new credentials to get the Kafka broker list, the admim URL and the api_key needed to authenticate the consumers or producers.\n\n ![](images/IES-IC-credentials.png)\n\n#### Event Streams Kafka Brokers\n\nRegardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.  These values can be acquired from the `kafka_brokers_sasl` section of the service instance's Service Credentials.\n\n```shell\nkubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project>\nkubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>\n```\n\n#### Event Streams API Key\n\nThe Event Streams Broker API Key is needed to connect any deployed consumers or producers to the service in IBM Cloud. To avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to.  This is available from the Service Credentials information you just created above.\n\n```shell\nkubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <target k8s namespace / ocp project>\nkubectl describe secret eventstreams-apikey -n <target k8s namespace / ocp project>\n```\n\n---\n\n### Using IBM Event Streams, deployed on RedHat OpenShift Container Platform\n\n#### Service Deployment\n\nThe installation is documented in the [product documentation](https://ibm.github.io/event-streams/installing/installing-openshift/) and in our [own note here.](https://ibm-cloud-architecture.github.io/refarch-eda/deployments/eventstreams/)\n\n#### Event Streams Kafka Brokers\n\nRegardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.\n\n```shell\nkubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project>\nkubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>\n```\n\n#### Event Streams API Key\n\nThe Event Streams Broker API Key is needed to connect any deployed consumers or producers to the service running in your cluster. To avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to.  You can specify keys at the topic and consumer group levels or use a unique key for all topics and all consumer groups.\n\n<InlineNotification kind=\"warning\">\n\n**IMPORTANT: The API Key must be created using the CLI** as it needs to grant certain transactional IDs for the Order Command component's producers to be able to transactionally produce messages to the topics it needs to.\n\n</InlineNotification>\n\nIn order to create the API Key through the CLI, you must install the Command Line Interface (CLI) explained in the IBM Event Streams post-installation instructions [here](https://ibm.github.io/event-streams/installing/post-installation/). Once you have the **cloudctl** CLI and the Event Streams plugin for it, you can create the API key as follows:\n\n```shell\ncloudctl es iam-service-id-create <NAME> --role administrator --all-topics --all-groups --all-schemas --txnid \"order-1,error-1\" --description \"<DESCRIPTION>\"\n```\n\nwhere you can tweak the role, topics, groups, etc to be more restrictive but the **txnid** must be as described.\n\nOnce you run the command above and the service credentials are created and the API key generated, provide the API key in the secret below:\n\n```shell\nkubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <target k8s namespace / ocp project>\nkubectl describe secrets -n <target k8s namespace / ocp project>\n```\n\n#### Event Streams Certificates\n\nIf you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores to connect securely between your application components and the Kafka brokers.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.\n\n- From the **Connect to this cluster** tab on the landing page of your Event Streams installation, download both the **Java truststore** and the **PEM certificate**.\n- Create the Java truststore Secret:\n  - Command: `oc create secret generic <secret-name> --from-file=/path/to/downloaded/file.jks`\n  - Example: `oc create secret generic es-truststore-jks --from-file=/Users/osowski/Downloads/es-cert.jks`\n- Create the PEM certificate Secret:\n  - Command: `oc create secret generic <secret-name> --from-file=/path/to/downloaded/file.pem`\n  - Example: `oc create secret generic es-ca-pemfile --from-file=/Users/osowski/Downloads/es-cert.pem`\n\n---\n\n### Using community-based Kafka Helm charts, deployed locally in-cluster\n\nIf you simply want to deploy Kafka using the open source, community-supported Helm Charts, you can do so with the following commands.\n\n#### Environment Considerations\n\n**TODO** Needs update to account for ServiceAccount integration after `helm template` generation\n\n#### Service Deployment\n\n1. Add Bitnami Helm Repository:\n```shell\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n2. Update the Helm repository:\n```shell\nhelm repo update\n```\n3. Create a Kubernetes Namespace or OpenShift Project.\n```shell\nkubectl create namespace <target namespace>\n```\n4. Deploy Kafka and Zookeeper using the `bitnami/kafka` Helm Chart:\n```shell\nmkdir bitnami\nmkdir templates\nhelm fetch --untar --untardir bitnami 'bitnami/kafka'\nhelm template --name kafka --set persistence.enabled=false --set securityContext.enabled=false \\ \n  bitnami/kafka --namespace <target namespace> --output-dir templates\n(kubectl/oc) apply -f templates/kafka/charts/zookeeper/templates/\n(kubectl/oc) apply -f templates/kafka/templates\n```\nIt will take a few minutes to get the pods ready.\n\n### Kafka Brokers\n\nRegardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.\n\n```shell\nkubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project>\nkubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>\n```\n\n---\n\n## Postgresql\n\nThe [Container Manager microservice](https://github.com/ibm-cloud-architecture/refarch-kc-container-ms/) persists the Reefer Container inventory in a Postgresql database.  The deployment of Postgresql is only necessary to support the deployment of the Container Manager microservice.  If you are not deploying the Container Manager microservice, you do not need to deploy and configure a Postgresql service and database.\n\n### Using Postgresql, hosted on IBM Cloud\n\n#### Service Deployment\n\n To install the service, follow the [product documentation here](https://cloud.ibm.com/catalog/services/databases-for-postgresql).\n\n Once the service is deployed, you need to create some service credentials and retreive the following values for the different configurations:\n\n * `postgres.username`\n * `postgres.password`\n * `postgres.composed`, which will need to be mapped to a JDBC URL in the format of `jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=verify-full&sslfactory=org.postgresql.ssl.NonValidatingFactory` _(this will remove the `username` and `password` values from the default `composed` string)_\n\n ![](images/postgres-credentials.png)\n\n#### Creating Postgresql credentials as Kubernetes Secrets\n\n1. Applying the same approach as above, copy the Postgresql URL as defined in the Postegresql service credential and execute the following command:\n```shell\nkubectl create secret generic postgresql-url --from-literal=binding='<replace with postgresql-url>' -n <target k8s namespace / ocp project>\n```\n\n2. For the user:\n```shell\nkubectl create secret generic postgresql-user --from-literal=binding='ibm_cloud_c...' -n <target k8s namespace / ocp project>\n```\n\n3. For the user password:\n```shell\nkubectl create secret generic postgresql-pwd --from-literal=binding='<password from the service credential>.' -n <target k8s namespace / ocp project>\n```\n\n4. When running Postgresql through the IBM Cloud service, additional SSL certificates are required to communicate securely:\n    1. Install the IBM Cloud Database CLI Plugin:\n   ```shell\n   ibmcloud plugin install cloud-databases\n   ```\n    2. Get the certificate using the name of the postgresql service:\n  ```shell\n  ibmcloud cdb deployment-cacert $IC_POSTGRES_SERV > postgresql.crt\n  ```\n    3. Then add it into an environment variable\n  ```shell\n  export POSTGRESQL_CA_PEM=\"$(cat ./postgresql.crt)\"\n  ```\n    4. Then define a secret:\n  ```shell\n  kubectl create secret generic postgresql-ca-pem --from-literal=binding=\"$POSTGRESQL_CA_PEM\" -n browncompute\n  ```\n\n---\n\n### Using community-based Postgresql Helm charts, deployed locally in-cluster\n\nIf you simply want to deploy Postgresql using the open source, community-supported Helm Charts, you can do so with the following commands.\n\n#### Environment Considerations\n\nReference [Application Components Pre-reqs](application-components.md#openshift-container-platform-3-11) for details on creating the necessary ServiceAccount with required permissions, prior to deployment.\n\n#### Service Deployment\n\n1. Add Bitnami Helm Repository:\n```shell\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n2. Update the Helm repository:\n```shell\nhelm repo update\n```\n\n3. Create a Kubernetes Namespace or OpenShift Project (if not already created).\n```shell\nkubectl create namespace <target namespace>\n```\n4. Deploy Postgresql using the `bitnami/postgresql` Helm Chart:\n```shell\nmkdir bitnami\nmkdir templates\nhelm fetch --untar --untardir bitnami bitnami/postgresql\nhelm template --name postgre-db --set postgresqlPassword=supersecret \\\n  --set persistence.enabled=false --set serviceAccount.enabled=true \\ \n  --set serviceAccount.name=<existing service account> bitnami/postgresql \\ \n  --namespace <target namespace> --output-dir templates\n(kubectl/oc) apply -f templates/postgresql/templates\n```\n  It will take a few minutes to get the pods ready.\n\n#### Creating Postgresql credentials as Kubernetes Secrets\n\n* The `postgresql-url` needs to point to the in-cluster (non-headless) Kubernetes Service created as part of the deployment and should take the form of the deployment name with the suffix of `-postgresql`:\n\n ```shell\n kubectl get services | grep postgresql | grep -v headless\n kubectl create secret generic postgresql-url --from-literal=binding='jdbc:postgresql://<helm-release-name>-postgresql' -n <target k8s namespace / ocp project>\n ```\n\n* For the user:\n\n ```shell\n kubectl create secret generic postgresql-user --from-literal=binding='postgres' -n <target k8s namespace / ocp project>\n ```\n\n* For the user password:\n\n ```shell\n kubectl create secret generic postgresql-pwd --from-literal=binding='<password used in the helm template command>.' -n <target k8s namespace / ocp project>\n ```\n\n#### Service Debugging & Troubleshooting\n\nAccess to the in-container password can be made using the following command.  This should be the same value you passed in when you deployed the service.\n\n```\nexport POSTGRES_PASSWORD=$(kubectl get secret --namespace  <target namespace> postgre-db-postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode)\n```\n\nAnd then use the `psql` command line interface to interact with postgresql. For that, we use a Docker image as a client to the Postgresql server:\n\n```\nkubectl run postgre-db-postgresql-client --rm --tty -i --restart='Never' --namespace <target namespace> --image bitnami/postgresql:11.3.0-debian-9-r38 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" --command -- psql --host postgre-db-postgresql -U postgres -p 5432\n```\n\nTo connect to your database from outside the cluster execute the following commands:\n\n```\n    kubectl port-forward --namespace <target namespace> svc/postgre-db-postgresql 5432:5432 &&\\\n    PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U postgres -p 5432\n```\n\n## BPM\n\nThe containers microservice component of this Reefer Container EDA reference application can be integrated with a BPM process for the the maintenance of the containers. This BPM process will dispatch a field engineer so that the engineer can go to the reefer container to fix it. The process of scheduling an engineer and then completing the work can best be facilitated through a process based, structured workflow. We will be using IBM BPM on Cloud or Cloud Pak for Automation to best demonstrate the workflow. This workflow can be explored in detail [here](https://github.com/ibm-cloud-architecture/refarch-reefer-ml/tree/master/docs/bpm).\n\nIn order for the containers microservice to fire the BPM workflow, we need to provide the following information through Kubernetes configMaps and secrets:\n\n1. Provide in a configMap:\n   * the **BPM authentication login endpoint**\n   * the **BPM workflow endpoint**\n   * the **BPM anomaly event threshold**\n   * the **BPM authentication token time expiration**\n\n   ```shell\n   cat <<EOF | kubectl apply -f -\n   apiVersion: v1\n   kind: ConfigMap\n   metadata:\n     name: bpm-anomaly\n   data:\n     url: <replace with your BPM workflow endpoint>\n     login: <replace with your BPM authentication endpoint>\n     expiration: <replace with the number of second for the auth token to expire after>\n     anomalyThreshold: <replace with the number of anomaly events to receive before calling BPM>\n   EOF\n   ```\n\n2. Provide your BPM instance's **credentials** in a secret:\n\n   ```shell\n   kubectl create secret generic bpm-anomaly --from-literal=user='<replace with your BPM user>' --from-literal=password='<replace with your BPM password>' -n <target k8s namespace / ocp project>\n   kubectl describe secrets -n <target k8s namespace / ocp project>\n   ```\n\n**IMPORTANT:** The names for both the secret and configMap (`bpm-anomaly`) is the default the container microservice uses in its [helm chart](https://github.com/ibm-cloud-architecture/refarch-kc-container-ms/tree/master/SpringContainerMS/chart/springcontainerms). Make sure the name for the configMap and secret you create **match** the names you used in the containers microservice's helm chart.\n\nIf you do not have access to any BPM instance with this field engineer dispatching workflow, you can bypass the call to BPM by disabling such call in the container microservice component. For doing so, you can use the following container microservice's API endpoints:\n\n1. Enable BPM: [`http://<container_microservice_endpoint>/bpm/enable`](#bpm)\n2. Disable BPM: [`http://<container_microservice_endpoint>/bpm/disable`](#bpm)\n3. BPM status: [`http://<container_microservice_endpoint>/bpm/status`](#bpm)\n\nwhere `<container_microservice_endpoint>` is the route, ingress or nodeport service you associated to your container microservice component at deployment time.\n","type":"Mdx","contentDigest":"ef2f6fcb6d92022d0d534138cb439c38","counter":279,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Deployment of backing services","description":"Deployment of backing services"},"exports":{},"rawBody":"---\ntitle: Deployment of backing services\ndescription: Deployment of backing services\n---\n\n\n## Kafka & Event Streams\n\n### Using IBM Event Streams, hosted on IBM Cloud\n\n#### Services Deployment\n\nWe recommend to follow [our most recent lab](https://ibm-cloud-architecture.github.io/refarch-eda/technology/event-streams/es-cloud/) on how to provision an Event Streams intance on cloud.\n\n* In the *Manage* panel add the topics needed for the solution. We need at least the following:\n\n ![](images/IES-IC-topics.png)\n\n* In the Service Credentials tab, create new credentials to get the Kafka broker list, the admim URL and the api_key needed to authenticate the consumers or producers.\n\n ![](images/IES-IC-credentials.png)\n\n#### Event Streams Kafka Brokers\n\nRegardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.  These values can be acquired from the `kafka_brokers_sasl` section of the service instance's Service Credentials.\n\n```shell\nkubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project>\nkubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>\n```\n\n#### Event Streams API Key\n\nThe Event Streams Broker API Key is needed to connect any deployed consumers or producers to the service in IBM Cloud. To avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to.  This is available from the Service Credentials information you just created above.\n\n```shell\nkubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <target k8s namespace / ocp project>\nkubectl describe secret eventstreams-apikey -n <target k8s namespace / ocp project>\n```\n\n---\n\n### Using IBM Event Streams, deployed on RedHat OpenShift Container Platform\n\n#### Service Deployment\n\nThe installation is documented in the [product documentation](https://ibm.github.io/event-streams/installing/installing-openshift/) and in our [own note here.](https://ibm-cloud-architecture.github.io/refarch-eda/deployments/eventstreams/)\n\n#### Event Streams Kafka Brokers\n\nRegardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.\n\n```shell\nkubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project>\nkubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>\n```\n\n#### Event Streams API Key\n\nThe Event Streams Broker API Key is needed to connect any deployed consumers or producers to the service running in your cluster. To avoid sharing security keys, create a Kubernetes Secret in the target cluster you will deploy the application microservices to.  You can specify keys at the topic and consumer group levels or use a unique key for all topics and all consumer groups.\n\n<InlineNotification kind=\"warning\">\n\n**IMPORTANT: The API Key must be created using the CLI** as it needs to grant certain transactional IDs for the Order Command component's producers to be able to transactionally produce messages to the topics it needs to.\n\n</InlineNotification>\n\nIn order to create the API Key through the CLI, you must install the Command Line Interface (CLI) explained in the IBM Event Streams post-installation instructions [here](https://ibm.github.io/event-streams/installing/post-installation/). Once you have the **cloudctl** CLI and the Event Streams plugin for it, you can create the API key as follows:\n\n```shell\ncloudctl es iam-service-id-create <NAME> --role administrator --all-topics --all-groups --all-schemas --txnid \"order-1,error-1\" --description \"<DESCRIPTION>\"\n```\n\nwhere you can tweak the role, topics, groups, etc to be more restrictive but the **txnid** must be as described.\n\nOnce you run the command above and the service credentials are created and the API key generated, provide the API key in the secret below:\n\n```shell\nkubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <target k8s namespace / ocp project>\nkubectl describe secrets -n <target k8s namespace / ocp project>\n```\n\n#### Event Streams Certificates\n\nIf you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores to connect securely between your application components and the Kafka brokers.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.\n\n- From the **Connect to this cluster** tab on the landing page of your Event Streams installation, download both the **Java truststore** and the **PEM certificate**.\n- Create the Java truststore Secret:\n  - Command: `oc create secret generic <secret-name> --from-file=/path/to/downloaded/file.jks`\n  - Example: `oc create secret generic es-truststore-jks --from-file=/Users/osowski/Downloads/es-cert.jks`\n- Create the PEM certificate Secret:\n  - Command: `oc create secret generic <secret-name> --from-file=/path/to/downloaded/file.pem`\n  - Example: `oc create secret generic es-ca-pemfile --from-file=/Users/osowski/Downloads/es-cert.pem`\n\n---\n\n### Using community-based Kafka Helm charts, deployed locally in-cluster\n\nIf you simply want to deploy Kafka using the open source, community-supported Helm Charts, you can do so with the following commands.\n\n#### Environment Considerations\n\n**TODO** Needs update to account for ServiceAccount integration after `helm template` generation\n\n#### Service Deployment\n\n1. Add Bitnami Helm Repository:\n```shell\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n2. Update the Helm repository:\n```shell\nhelm repo update\n```\n3. Create a Kubernetes Namespace or OpenShift Project.\n```shell\nkubectl create namespace <target namespace>\n```\n4. Deploy Kafka and Zookeeper using the `bitnami/kafka` Helm Chart:\n```shell\nmkdir bitnami\nmkdir templates\nhelm fetch --untar --untardir bitnami 'bitnami/kafka'\nhelm template --name kafka --set persistence.enabled=false --set securityContext.enabled=false \\ \n  bitnami/kafka --namespace <target namespace> --output-dir templates\n(kubectl/oc) apply -f templates/kafka/charts/zookeeper/templates/\n(kubectl/oc) apply -f templates/kafka/templates\n```\nIt will take a few minutes to get the pods ready.\n\n### Kafka Brokers\n\nRegardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes ConfigMap needs to be created to support the deployments of the application's microservices.  These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment.\n\n```shell\nkubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <target k8s namespace / ocp project>\nkubectl describe configmap kafka-brokers -n <target k8s namespace / ocp project>\n```\n\n---\n\n## Postgresql\n\nThe [Container Manager microservice](https://github.com/ibm-cloud-architecture/refarch-kc-container-ms/) persists the Reefer Container inventory in a Postgresql database.  The deployment of Postgresql is only necessary to support the deployment of the Container Manager microservice.  If you are not deploying the Container Manager microservice, you do not need to deploy and configure a Postgresql service and database.\n\n### Using Postgresql, hosted on IBM Cloud\n\n#### Service Deployment\n\n To install the service, follow the [product documentation here](https://cloud.ibm.com/catalog/services/databases-for-postgresql).\n\n Once the service is deployed, you need to create some service credentials and retreive the following values for the different configurations:\n\n * `postgres.username`\n * `postgres.password`\n * `postgres.composed`, which will need to be mapped to a JDBC URL in the format of `jdbc:postgresql://<hostname>:<port>/<database-name>?sslmode=verify-full&sslfactory=org.postgresql.ssl.NonValidatingFactory` _(this will remove the `username` and `password` values from the default `composed` string)_\n\n ![](images/postgres-credentials.png)\n\n#### Creating Postgresql credentials as Kubernetes Secrets\n\n1. Applying the same approach as above, copy the Postgresql URL as defined in the Postegresql service credential and execute the following command:\n```shell\nkubectl create secret generic postgresql-url --from-literal=binding='<replace with postgresql-url>' -n <target k8s namespace / ocp project>\n```\n\n2. For the user:\n```shell\nkubectl create secret generic postgresql-user --from-literal=binding='ibm_cloud_c...' -n <target k8s namespace / ocp project>\n```\n\n3. For the user password:\n```shell\nkubectl create secret generic postgresql-pwd --from-literal=binding='<password from the service credential>.' -n <target k8s namespace / ocp project>\n```\n\n4. When running Postgresql through the IBM Cloud service, additional SSL certificates are required to communicate securely:\n    1. Install the IBM Cloud Database CLI Plugin:\n   ```shell\n   ibmcloud plugin install cloud-databases\n   ```\n    2. Get the certificate using the name of the postgresql service:\n  ```shell\n  ibmcloud cdb deployment-cacert $IC_POSTGRES_SERV > postgresql.crt\n  ```\n    3. Then add it into an environment variable\n  ```shell\n  export POSTGRESQL_CA_PEM=\"$(cat ./postgresql.crt)\"\n  ```\n    4. Then define a secret:\n  ```shell\n  kubectl create secret generic postgresql-ca-pem --from-literal=binding=\"$POSTGRESQL_CA_PEM\" -n browncompute\n  ```\n\n---\n\n### Using community-based Postgresql Helm charts, deployed locally in-cluster\n\nIf you simply want to deploy Postgresql using the open source, community-supported Helm Charts, you can do so with the following commands.\n\n#### Environment Considerations\n\nReference [Application Components Pre-reqs](application-components.md#openshift-container-platform-3-11) for details on creating the necessary ServiceAccount with required permissions, prior to deployment.\n\n#### Service Deployment\n\n1. Add Bitnami Helm Repository:\n```shell\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n2. Update the Helm repository:\n```shell\nhelm repo update\n```\n\n3. Create a Kubernetes Namespace or OpenShift Project (if not already created).\n```shell\nkubectl create namespace <target namespace>\n```\n4. Deploy Postgresql using the `bitnami/postgresql` Helm Chart:\n```shell\nmkdir bitnami\nmkdir templates\nhelm fetch --untar --untardir bitnami bitnami/postgresql\nhelm template --name postgre-db --set postgresqlPassword=supersecret \\\n  --set persistence.enabled=false --set serviceAccount.enabled=true \\ \n  --set serviceAccount.name=<existing service account> bitnami/postgresql \\ \n  --namespace <target namespace> --output-dir templates\n(kubectl/oc) apply -f templates/postgresql/templates\n```\n  It will take a few minutes to get the pods ready.\n\n#### Creating Postgresql credentials as Kubernetes Secrets\n\n* The `postgresql-url` needs to point to the in-cluster (non-headless) Kubernetes Service created as part of the deployment and should take the form of the deployment name with the suffix of `-postgresql`:\n\n ```shell\n kubectl get services | grep postgresql | grep -v headless\n kubectl create secret generic postgresql-url --from-literal=binding='jdbc:postgresql://<helm-release-name>-postgresql' -n <target k8s namespace / ocp project>\n ```\n\n* For the user:\n\n ```shell\n kubectl create secret generic postgresql-user --from-literal=binding='postgres' -n <target k8s namespace / ocp project>\n ```\n\n* For the user password:\n\n ```shell\n kubectl create secret generic postgresql-pwd --from-literal=binding='<password used in the helm template command>.' -n <target k8s namespace / ocp project>\n ```\n\n#### Service Debugging & Troubleshooting\n\nAccess to the in-container password can be made using the following command.  This should be the same value you passed in when you deployed the service.\n\n```\nexport POSTGRES_PASSWORD=$(kubectl get secret --namespace  <target namespace> postgre-db-postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode)\n```\n\nAnd then use the `psql` command line interface to interact with postgresql. For that, we use a Docker image as a client to the Postgresql server:\n\n```\nkubectl run postgre-db-postgresql-client --rm --tty -i --restart='Never' --namespace <target namespace> --image bitnami/postgresql:11.3.0-debian-9-r38 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" --command -- psql --host postgre-db-postgresql -U postgres -p 5432\n```\n\nTo connect to your database from outside the cluster execute the following commands:\n\n```\n    kubectl port-forward --namespace <target namespace> svc/postgre-db-postgresql 5432:5432 &&\\\n    PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U postgres -p 5432\n```\n\n## BPM\n\nThe containers microservice component of this Reefer Container EDA reference application can be integrated with a BPM process for the the maintenance of the containers. This BPM process will dispatch a field engineer so that the engineer can go to the reefer container to fix it. The process of scheduling an engineer and then completing the work can best be facilitated through a process based, structured workflow. We will be using IBM BPM on Cloud or Cloud Pak for Automation to best demonstrate the workflow. This workflow can be explored in detail [here](https://github.com/ibm-cloud-architecture/refarch-reefer-ml/tree/master/docs/bpm).\n\nIn order for the containers microservice to fire the BPM workflow, we need to provide the following information through Kubernetes configMaps and secrets:\n\n1. Provide in a configMap:\n   * the **BPM authentication login endpoint**\n   * the **BPM workflow endpoint**\n   * the **BPM anomaly event threshold**\n   * the **BPM authentication token time expiration**\n\n   ```shell\n   cat <<EOF | kubectl apply -f -\n   apiVersion: v1\n   kind: ConfigMap\n   metadata:\n     name: bpm-anomaly\n   data:\n     url: <replace with your BPM workflow endpoint>\n     login: <replace with your BPM authentication endpoint>\n     expiration: <replace with the number of second for the auth token to expire after>\n     anomalyThreshold: <replace with the number of anomaly events to receive before calling BPM>\n   EOF\n   ```\n\n2. Provide your BPM instance's **credentials** in a secret:\n\n   ```shell\n   kubectl create secret generic bpm-anomaly --from-literal=user='<replace with your BPM user>' --from-literal=password='<replace with your BPM password>' -n <target k8s namespace / ocp project>\n   kubectl describe secrets -n <target k8s namespace / ocp project>\n   ```\n\n**IMPORTANT:** The names for both the secret and configMap (`bpm-anomaly`) is the default the container microservice uses in its [helm chart](https://github.com/ibm-cloud-architecture/refarch-kc-container-ms/tree/master/SpringContainerMS/chart/springcontainerms). Make sure the name for the configMap and secret you create **match** the names you used in the containers microservice's helm chart.\n\nIf you do not have access to any BPM instance with this field engineer dispatching workflow, you can bypass the call to BPM by disabling such call in the container microservice component. For doing so, you can use the following container microservice's API endpoints:\n\n1. Enable BPM: [`http://<container_microservice_endpoint>/bpm/enable`](#bpm)\n2. Disable BPM: [`http://<container_microservice_endpoint>/bpm/disable`](#bpm)\n3. BPM status: [`http://<container_microservice_endpoint>/bpm/status`](#bpm)\n\nwhere `<container_microservice_endpoint>` is the route, ingress or nodeport service you associated to your container microservice component at deployment time.\n","fileAbsolutePath":"/home/runner/work/refarch-kc/refarch-kc/docs/src/pages/deployments/backing-services.mdx"}}}}