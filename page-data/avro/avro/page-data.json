{"componentChunkName":"component---src-pages-avro-avro-mdx","path":"/avro/avro/","result":{"pageContext":{"frontmatter":{"title":"Apache Avro","description":"Apache Avro"},"relativePagePath":"/avro/avro.mdx","titleType":"append","MdxNode":{"id":"1c5e2944-147b-531d-b393-6400f7309d42","children":[],"parent":"3667d095-8ffd-5621-b78d-388d0686136a","internal":{"content":"---\ntitle: Apache Avro\ndescription: Apache Avro\n---\n\n## Introduction\n\nHere we explain the [Apache Avro](https://avro.apache.org/) messaging integration we have done in one of our integration tests for the **refarch-kc-container-ms** component, which is part of the [Reefer Containers reference implementation](https://ibm-cloud-architecture.github.io/refarch-kc/) of the [IBM Event Driven Architectures reference architecture](https://ibm-cloud-architecture.github.io/refarch-eda/). The Reefer Containers reference implementation is a simulation of what a container shipment process could look like in reality. From a manufacturer creating some goods to the delivery of those to a retailer, going through requesting a container, loading the goods into the container, finding a voyage for that container on a ship, monitoring the container's temperature and GPS location, delivering the container, unloading the goods, etc. As you can imagine, this scenario is ideal for an Event Driven architecture where we not only have a microservices based application but also the integration of these using Event Driven Architecture components (such as Kafka) and patterns (such as Saga, CQRS, Event Sourcing, etc).\n\n### What is Apache Avro\n\nAvro is an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.\n\n### Why Apache Avro\n\nThere are several websites that discuss the Apache Avro data serialization system benefits over other messaging data protocols. A simple google search will list dozens of them. Here, we will highlight just a few from a [Confluent blog post](https://www.confluent.io/blog/avro-kafka-data/):\n\n- It has a direct mapping to and from JSON\n- It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.\n- It is very fast.\n- It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.\n- It has a rich, extensible schema language defined in pure JSON\n- It has the best notion of compatibility for evolving your data over time.\n\n## Avro, Kafka and Schema Registry\n\nAvro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format. Schema Registry defines a scope in which schemas can evolve, and that scope is the subject. The name of the subject depends on the configured subject name strategy, which by default is set to derive subject name from topic name.\n\nIn our case, this Avro data are messages sent to a kafka topic. Each message is a key-value pair. Either the message key or the message value, or both, can be serialized as Avro. Integration with Schema Registry means that Kafka messages do not need to be written with the entire Avro schema. Instead, Kafka messages are written with the **schema id**. The producers writing the messages and the consumers reading the messages must be using the same Schema Registry to get the same mapping between a schema and schema id.\n\nKafka is used as Schema Registry storage backend. The special Kafka topic `<kafkastore.topic>` (default `_schemas`), with a single partition, is used as a highly available write ahead log. All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log. A Schema Registry instance therefore both produces and consumes messages under the `_schemas` topic. It produces messages to the log when, for example, new schemas are registered under a subject, or when updates to compatibility settings are registered. Schema Registry consumes from the `_schemas` log in a background thread, and updates its local caches on consumption of each new `_schemas` message to reflect the newly added schema or compatibility setting. Updating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability.\n\n### How does it all work\n\n![schema registry management](images/schema-registry-and-kafka.png)\n\nWhen the producer sends a message/event to a Kafka topic for the first time, it sends the schema for that message/event to the Schema Registry. The Schema Registry registers this schema to the subject for the Kafka topic we want to send the message/event to, and returns the **schema id** to the producer. The producer **caches this mapping between the schema and schema id** for subsequent message writes, so **it only contacts Schema Registry on the first message/event write** (unless the schema has changed, that is evolved, when the schema registry will be contacted again for validation and storage of this new version of the schema). Kafka messages are written along with the **schema id** rather than with the entire data schema.\n\nWhen a consumer reads this data, it sees the **Avro schema id and sends a schema request to Schema Registry**. Schema Registry retrieves the schema associated to that schema id, and **returns the schema to the consumer**. The consumer **caches** this mapping between the schema and schema id for subsequent message reads, so it only contacts Schema Registry on the first schema id read.\n\n## Our implementation\n\nAs mentioned in the introduction, the integration of the Apache Avro data serialization system has been done in one of the integration test for the `refarch-kc-container-ms` component of the [Reefer Containers reference implementation](https://ibm-cloud-architecture.github.io/refarch-kc/) of the [IBM Event Driven Architectures reference architecture](https://ibm-cloud-architecture.github.io/refarch-eda/).\n\nThe **refarch-kc-container-ms** component will take care of the reefer containers status. From adding new reefers to the available containers list to assigning a container to a particular order and managing the status of that reefer throughout the shipment process aforementioned. \n\nThe integration tests for our Reefer Containers reference implementation can be found [here](https://github.com/ibm-cloud-architecture/refarch-kc/tree/master/itg-tests). The integration tests are being developed in python and their main goal is to validate the successful deployment of the Reefer Containers reference implementation end-to-end.\n\nThe particular integration test (still under development) where we have integrated the Apache Avro serialization system can be found under the `ContainersPython` folder. More precisely, these are the files and folders involved in our implementation:\n\n```shell\n├── data_schemas/avro_test\n│   ├── container_event.avsc\n│   ├── container_event_key.avsc\n│   ├── container_event_payload.avsc\n│   ├── container_event_type.avsc\n│   ├── container_status.avsc\n│   └── utils\n│       └── avroEDAUtils.py\n└── itg-tests\n    ├── ContainersPython\n    │   ├── ConsumeAvroContainers.py\n    │   └── ContainerAvroProducer.py\n    └── kafka\n        ├── KcAvroConsumer.py\n        └── KcAvroProducer.py\n```\n\nthat will allow us to send **container events** into the **containers** Kafka topic and read from such topic.\n\nBy using these python scripts, we will be able to validate:\n\n- Sending/Receiving **Apache Avro encoded messages**.\n- **Apache Avro data schema definitions** for data correctness.\n- **Schema Registry** for Apache Avro data schema management.\n\n### Data Schemas\n\nAvro schemas are defined with JSON. An example of a Container Event for creating a new reefer container to the available list of containers for our reference application looks like:\n\n```json\n{\n  \"containerID\": \"container01\",\n  \"timestamp\": 1569410690,\n  \"type\": \"ContainerAdded\",\n  \"payload\": {\n    \"containerID\": \"container01\",\n    \"type\": \"Reefer\",\n    \"status\": \"Empty\",\n    \"latitude\": 37.8,\n    \"longitude\": -122.25,\n    \"capacity\": 110,\n    \"brand\": \"itg-brand\"\n  }\n}\n```\n\nAn Avro schema could be a nested schema which allows us to have a smaller reusable data schemas to define bigger and more complex ones. This is the case for our Container Event data schema. For instance, the payload is defined on its own data schema (`container_event_payload.avsc`) which the Container Event data schema refers to:\n\n```json\n{\n  \"namespace\": \"ibm.eda.kc.container.event\",\n  \"name\": \"payload\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"containerID\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"type\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"status\",\n      \"type\": \"ibm.eda.kc.container.status\"\n    },\n    {\n      \"name\": \"latitude\",\n      \"type\": \"float\"\n    },\n    {\n      \"name\": \"longitude\",\n      \"type\": \"float\"\n    },\n    {\n      \"name\": \"capacity\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"brand\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\nAs you can see, the status attribute of the payload is yet another data schema itself which, in this case, is of type enum:\n\n```json\n{\n  \"namespace\": \"ibm.eda.kc.container\",\n  \"name\": \"status\",\n  \"type\": \"enum\",\n  \"symbols\": [\n    \"Loaded\",\n    \"Empty\",\n    \"Unassignable\",\n    \"PartiallyLoaded\"\n  ]\n}\n```\n\nAll the different data schemas for a Container Event can be found under the `data_schemas/avro_test` folder.\n\nIn that folder we have also developed a util script (`avroEDAUtils.py`) to be able to construct the final Container Event data schema that is needed by our producer:\n\n```python\ndef getContainerEventSchema(schema_files_location):\n  # Read all the schemas needed in order to produce the final Container Event Schema\n  known_schemas = avro.schema.Names()\n  container_status_schema = LoadAvsc(schema_files_location + \"container_status.avsc\", known_schemas)\n  container_event_payload_schema = LoadAvsc(schema_files_location + \"container_event_payload.avsc\", known_schemas)\n  container_event_type_schema = LoadAvsc(schema_files_location + \"container_event_type.avsc\", known_schemas)\n  container_event_schema = LoadAvsc(schema_files_location + \"container_event.avsc\", known_schemas)\n  return container_event_schema\n\ndef LoadAvsc(file_path, names=None):\n  # Load avsc file\n  # file_path: path to schema file\n  # names(optional): avro.schema.Names object\n  file_text = open(file_path).read()\n  json_data = json.loads(file_text)\n  schema = avro.schema.SchemaFromJSONData(json_data, names)\n  return schema\n```\n\n#### See it in action\n\nHere, we are going to see how data schemas help with data correctness. Using the payload for our container messages/events as the example, this is the output of a correct message being sent:\n\n```\n--- Container event to be published: ---\n{\n  \"containerID\": \"container01\",\n  \"type\": \"Reefer\",\n  \"status\": \"Empty\",\n  \"latitude\": 37.8,\n  \"longitude\": -122.25,\n  \"capacity\": 110,\n  \"brand\": \"itg-brand\"\n}\n\nMessage delivered to containers [0]\n```\n\nHowever, if we try to send a payload where, for instance the container ID is an integer rather than a string, we will get an `avro.io.AvroTypeException`:\n\n```\navro.io.AvroTypeException: The datum {'containerID': 12345, 'type': 'Reefer', 'status': 'Empty', 'latitude': 37.8, 'longitude': -122.25, 'capacity': 110, 'brand': 'itg-brand'} is not an example of the schema\n{\n    \"type\": {\n        \"type\": \"record\",\n        \"name\": \"payload\",\n        \"namespace\": \"ibm.eda.kc.container.event\",\n        \"fields\": [\n          {\n            \"type\": \"string\",\n            \"name\": \"containerID\"\n          },\n          {\n            \"type\": \"string\",\n            \"name\": \"type\"\n          },\n          {\n            \"type\": {\n              \"type\": \"enum\",\n              \"name\": \"status\",\n              \"namespace\": \"ibm.eda.kc.container\",\n              \"symbols\": [\n                \"Loaded\",\n                \"Empty\",\n                \"Unassignable\",\n                \"PartiallyLoaded\"\n              ]\n            },\n            \"name\": \"status\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"latitude\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"longitude\"\n          },\n          {\n            \"type\": \"int\",\n            \"name\": \"capacity\"\n          },\n          {\n            \"type\": \"string\",\n            \"name\": \"brand\"\n          }\n        ]\n      }\n```\n\n### Producer and Consumer\n\nThe python scripts developed to implement a producer and consumer to a kafka topic that sends Avro messages whose data schemas are managed by a schema registry are:\n\n```shell\n└── itg-tests\n    ├── ContainersPython\n    │   ├── ConsumeAvroContainers.py\n    │   └── ContainerAvroProducer.py\n    └── kafka\n        ├── KcAvroConsumer.py\n        └── KcAvroProducer.py\n```\n\nWe have used the **confluent_kafka avro libraries** to implement our producer and consumer.\n\n```python\nfrom confluent_kafka.avro import AvroProducer, AvroConsumer\n```\n\n#### Producer\n\nWe create our `KafkaProducer` object where we define some of the **AvroProducer** options such as the schema registry url for data schema registration and management. But it is not until we call the `prepareProducer` method that we actually create the **AvroProducer** with that **schema registry** to be used as well as the **data schemas for the key and value** of our Container Event to be sent.\n\nFinally, in the `publishEvent` method we send a value plus a key to a kafka topic.\n\nproducer when we call `prepareProducer`\n\n```python\nimport json\nfrom confluent_kafka import KafkaError\nfrom confluent_kafka.avro import AvroProducer\n\nclass KafkaProducer:\n\n    def __init__(self,kafka_brokers = \"\",kafka_apikey = \"\",schema_registry_url = \"\"):\n        self.kafka_brokers = kafka_brokers\n        self.kafka_apikey = kafka_apikey\n        self.schema_registry_url = schema_registry_url\n\n    def prepareProducer(self,groupID = \"pythonproducers\",key_schema = \"\", value_schema = \"\"):\n        options ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'schema.registry.url': self.schema_registry_url,\n                'group.id': groupID\n        }\n        self.producer = AvroProducer(options,default_key_schema=key_schema,default_value_schema=value_schema)\n\n    def publishEvent(self, topicName, value, key):\n        # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first\n        self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(value)[key], callback=self.delivery_report)\n        self.producer.flush()\n```\n\nTo use this class you need to do the following steps:\n\n```python\n# load schema definitions for key and value\nfrom utils.avroEDAUtils import getContainerEventSchema, getContainerKeySchema\ncontainer_event_value_schema = getContainerEventSchema(\"/data_schemas/avro_test/\")\ncontainer_event_key_schema = getContainerKeySchema(\"/data_schemas/avro_test/\")\n# Create a producer with the schema registry URL end point\nkp = KafkaProducer(KAFKA_ENV,KAFKA_BROKERS,KAFKA_APIKEY,SCHEMA_REGISTRY_URL)\nkp.prepareProducer(\"ContainerProducerPython\",container_event_key_schema,container_event_value_schema)\n# loop on publishing events\nkp.publishEvent(TOPIC_NAME,container_event)\n```\n\n#### Consumer\n\nSimilarly to the producer, when we create a `KafkaConsumer` object we are just setting some of its attributes such as the kafka topic we will listen to and the schema registry url the producer will retrieve the data schemas from based on the schema ids messages comes with. It is only when we call the `prepareConsumer` method that we actually create the **AvroConsumer** and subscribe it to the intended kafka topic.\n\n```python\nimport json\nfrom confluent_kafka.avro import AvroConsumer\n\nclass KafkaConsumer:\n\n    def __init__(self, kafka_brokers = \"\", kafka_apikey = \"\", topic_name = \"\", schema_registry_url = \"\"):\n        self.kafka_brokers = kafka_brokers\n        self.kafka_apikey = kafka_apikey\n        self.topic_name = topic_name\n        self.schema_registry_url = schema_registry_url \n\n    def prepareConsumer(self, groupID = \"pythonconsumers\"):\n        options ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'group.id': groupID,\n                'auto.offset.reset': 'earliest',\n                'schema.registry.url': self.schema_registry_url,\n        }\n        self.consumer = AvroConsumer(options)\n        self.consumer.subscribe([self.topic_name])\n\n# ...\n```\n\n### Schema registry\n\nFor now, we have used the [Confluent schema registry](https://hub.docker.com/r/confluentinc/cp-schema-registry/) for our work although our goal is to use [IBM Event Streams](https://ibm.github.io/event-streams/schemas/overview/).\n\nThe integration of the schema registry with your kafka broker is quite easy. In fact, all you need is to provide the schema registry with your zookeeper cluster url and give your schema registry a hostname: https://docs.confluent.io/current/installation/docker/config-reference.html#schema-registry-configuration\n\nOnce you have your schema registry up and running, this provides a rich API endpoint to operate with: https://docs.confluent.io/current/schema-registry/using.html#common-sr-usage-examples\n\nFor example:\n\nLet's assume we have created a new kafka topic called `avrotest` for testing our work. And let's also assume we are sending persona messages/events whose data schema is the following:\n\n```json\n{\n    \"namespace\": \"avro.test\",\n    \"name\": \"persona\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        }\n    ]\n }\n```\n\n- Get the **subjects** (that is, the kafka topics to which we have a schema registered against. As explained before, we either have registered the schema manually ourselves or the Avro producer has registered it when we have sent the first message)\n\n  ```http\n  curl -X GET http://localhost:8081/subjects\n\n  [\"avrotest-value\",\"avrotest-key\"]\n  ```\n\n- Get versions for a subject:\n\n  ```http\n  curl -X GET http://localhost:8081/subjects/avrotest-value/versions\n\n  [1]\n  ```\n\n- Get a specific version:\n\n  ```http\n  curl -X GET http://localhost:8081/subjects/avrotest-value/versions/1/\n\n  {\n    \"subject\": \"avrotest-value\",\n    \"version\": 1,\n    \"id\": 1,\n    \"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"persona\\\",\\\"namespace\\\":\\\"avro.test\\\",\\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"}]}\"\n  }\n  ```\n\n- Get the schema of a specific subject version:\n\n  ```http\n  curl -X GET http://localhost:8081/subjects/avrotest-value/versions/1/schema\n\n  {\n    \"type\": \"record\",\n    \"name\": \"persona\",\n    \"namespace\": \"avro.test\",\n    \"fields\": [\n      {\n        \"name\": \"name\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"age\",\n        \"type\": \"int\"\n      },\n      {\n        \"name\": \"gender\",\n        \"type\": \"string\"\n      }\n    ]\n  }\n  ```\n\n- Get the schema of a specific subject latest version:\n\n  ```http\n  curl -X GET http://localhost:8081/subjects/avrotest-value/versions/latest/schema\n\n  {\n    \"type\": \"record\",\n    \"name\": \"persona\",\n    \"namespace\": \"avro.test\",\n    \"fields\": [\n      {\n        \"name\": \"name\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"age\",\n        \"type\": \"int\"\n      },\n      {\n        \"name\": \"gender\",\n        \"type\": \"string\"\n      }\n    ]\n  }\n  ```\n\n### Data evolution\n\nSo far we  have seen what Avro is, what a data schema is, what a schema registry is and how this all works together. From creating a data schema for your messages/events to comply with to how the schema registry and data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their schemas to the rich API the Confluent schema registry provides to interact with.\n\nHowever, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying [Event Storming](https://ibm-cloud-architecture.github.io/refarch-eda/methodology/eventstorming/) or [Domain Driven Design](https://ibm-cloud-architecture.github.io/refarch-eda/methodology/ddd/) for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data like your use or business cases may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases.\n\nBut it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event broker) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the event source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to [hundreds of years](https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/)) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, data at the end of the day.\n\nThere are mainly three types of data compatibility:\n\n1. Backward\n2. Forward\n3. Full\n\n#### Backward compatibility\n\nBackward compatibility means that **consumers using the new schema can read data produced with the last schema**.\n\nUsing the persona data schema already mentioned throughout this readme, what if we decide to change the data schema to add a new attribute such as place of birth? That is, the new schema would look like:\n\n```json\n{\n    \"namespace\": \"avro.test\",\n    \"name\": \"persona\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"place_of_birth\",\n            \"type\" : \"string\"\n        }\n    ]\n}\n```\n\nhere is the output when we try to produce an event/message with the above data schema:\n\n```python\n### Persona event to be published: ###\n{'name': 'david', 'age': '25', 'gender': 'male', 'place_of_birth': 'USA'}\n######################################\nTraceback (most recent call last):\n  File \"ContainerAvroProducer.py\", line 73, in <module>\n    kp.publishEvent(TOPIC_NAME,container_event,\"1\")\n  File \"/home/kafka/KcAvroProducer.py\", line 42, in publishEvent\n    self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(key), callback=self.delivery_report)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\", line 80, in produce\n    value = self._serializer.encode_record_with_schema(topic, value_schema, value)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 105, in encode_record_with_schema\n    schema_id = self.registry_client.register(subject, schema)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/cached_schema_registry_client.py\", line 223, in register\n    raise ClientError(\"Invalid Avro schema:\" + str(code))\nconfluent_kafka.avro.error.ClientError: Invalid Avro schema:422\n```\n\nAnd the reason for such error is that, because new schemas must be backward compatible (default compatibility mode for Confluent kafka data schemas topics), we can't just simply add a new attribute. Consumers using the new schema must be able to read data produced with the last schema. That is, if a consumer was to read old messages with the schema above, it would expect the `place_of_birth` attribute and its value on these old messages. However, the messages were produced with the old schema that did not enforce such attribute. Hence, the problem.\n\nWe can also check the compatibility of this new schema using the API:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\"}]}\"}' \n        http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":false}\n```\n\nHow do we evolve our schema to add new attributes in a way that the schema is _BACKWARD_ compatible? Adding a **default** value for such attribute so the consumer can use it when reading old messages that were produced without that attribute:\n\n```json\n{\n    \"namespace\": \"avro.test\",\n    \"name\": \"persona\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"place_of_birth\",\n            \"type\" : \"string\",\n            \"default\": \"nonDefined\"\n        }\n    ]\n}\n```\n\nRather than changing it straight in the code, we can do some sort of validation through the API:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" \n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nWe now can evolve our data schema to enforce a new attribute with our new messages/events being produced but making sure the consumer is able to read old messages that do not contain such attribute. We do so by sending a new persona event/message along with this new data schema. This will make the schema registry to register the new data schema.\n\nWe can validate the new data schema version has been registered by using the schema registry API:\n\n```http\n\ncurl -X GET http://localhost:8081/subjects/avrotest-value/versions\n\n[1,2]\n\ncurl -X GET http://localhost:8081/subjects/avrotest-value/versions/latest/schema\n\n{\"type\":\"record\",\n \"name\":\"persona\",\n \"namespace\":\"avro.test\",\n \"fields\":[{\"name\":\"name\",\"type\":\"string\"},\n           {\"name\":\"age\",\"type\":\"int\"},\n           {\"name\":\"gender\",\"type\":\"string\"},\n           {\"name\":\"place_of_birth\",\"type\":\"string\",\"default\":\"nonDefined\"}]}\n```\n\nWhat if we want to remove an attribute from our persona events/messages now? Well, this one is easy since the Avro consumer will simply ignore/drop all those attributes in the old persona events/messages that are not defined in the new data schema and just take in those that are defined.\n\nLet's try to remove the `gender` attribute:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\n#### Forward compatibility\n\nForwards compatibility means that **data produced with a new schema can be read by consumers using the last schema**.\n\nFirst, let's set the compatibility type to _FORWARD_ (default compatibility mode in Confluent kafka is backward):\n\n```http\ncurl -X PUT -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"compatibility\": \"FORWARD\"}' http://localhost:8081/config\n\n{\"compatibility\":\"FORWARD\"}\n\n```\n\n```http\ncurl -X GET http://localhost:8081/config\n\n{\"compatibilityLevel\":\"FORWARD\"}\n```\n\nNow, how about removing an attribute when the compatibility type configured is set to _FORWARD_? In this case, it is not as simple as removing the attribute from the new schema as the consumer will expect such attribute that the producer will not add to the events/messages. Let's try to remove the `gender` attribute from the persona messages/events:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":false}\n```\n\nSo, how can we produce new persona events/messages (without the `gender` attribute) that are compatible with the last data schema used by consumers (that expects an attribute called gender)?\n\nThe trick here is to **first register an \"intermediate\" data schema that adds a default value** to `gender` if it is not defined. This way, the \"intermediate\" data schema will become the last data schema for the consumers and when we producer sent messages that do not contain the `gender` attribute, the consumer will know what to do:\n\nIntermediate schema:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"gender\\\",\\\"type\\\": \\\"string\\\",\\\"default\\\": \\\"nonProvided\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nWe register this data schema either by sending it along with a message/event using our producer or we simply register it using the schema registry API. Once we have this \"intermediate\" schema registered that will actually become the last data schema for the consumer, we check if our end goal data schema without the `gender` attribute is forward compatible or not:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nIf we send a persona message that does not contain the `gender` attribute now, we should succeed:\n\n```python\n### Persona event to be published: ###\n{'name': 'david', 'age': 25, 'place_of_birth': 'USA'}\n######################################\nMessage delivered to avrotest [0]\n```\n\nContrary to the backward compatibility, in forward compatibility, adding a new attribute to your events/messages is not a problem because the consumers will simply ignore/drop this new attribute since the schema they are still using (the last one) does not include it. So let's say we want to add a new attribute called `hair` to represent the color of a persona's hair:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"},\n                                 {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nWe see there is no problem at all and if we try to send a message/event containing this new attribute along with the new schema:\n\n```python\n### Persona event to be published: ###\n{'name': 'John', 'age': 25, 'place_of_birth': 'London', 'hair': 'brown'}\n######################################\nMessage delivered to avrotest [0]\n```\n\nThe new data schema is registered and new messages/events complying with that new data schema are sent with no problem at all.\n\n#### Full compatibility\n\nFull compatibility means **data schemas are both backward and forward compatible**. Data schemas evolve in a fully compatible way: **old data can be read with the new data schema, and new data can also be read with the last data schema**.\n\nIn some data formats, such as JSON, there are no full-compatible changes. Every modification is either only forward or only backward compatible. But in other data formats, like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change.\n\nSo let's see if we can delete the `place_of_birth` attribute, the only attribute in our data schema that defines a default value:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"}]}\"}' \nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nIt looks like it may work. Let's send a message without that attribute along with the new data schema:\n\n```python\n### Persona event to be published: ###\n{'name': 'John', 'age': 25, 'hair': 'brown'}\n######################################\nMessage delivered to avrotest [0]\n```\n\nLet's now try to add an attribute with a default value. Let's say we want to add an attribute for the `hobbies` of a persona whose default value will be `none`\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"hobbies\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"none\\\"}]}\"}'\nßhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nLet's send a message along with the new data schema to be completely sure:\n\n```python\n### Persona event to be published: ###\n{'name': 'John', 'age': 25, 'hair': 'brown', 'hobbies': 'dance,music,food'}\n######################################\nMessage delivered to avrotest [0]\n```\n\nAs expected, it did work.\n\nWe now know how a data schema can evolve when full compatibility is required. That is, we know what attributes can be removed and how to add new attributes.\n\n## Compendium\n\nHere are some links we have visited to carry out our work and found interesting to read:\n\n* [https://www.confluent.io/blog/avro-kafka-data/](https://www.confluent.io/blog/avro-kafka-data/)\n\n* [https://avro.apache.org/docs/current/](https://avro.apache.org/docs/current/)\n\n* [https://docs.confluent.io/current/schema-registry/index.html](https://docs.confluent.io/current/schema-registry/index.html)\n\n* [https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html#schema-registry-tutorial-definition](https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html#schema-registry-tutorial-definition)\n\n* [https://docs.oracle.com/database/nosql-11.2.2.0/GettingStartedGuide/schemaevolution.html](https://docs.oracle.com/database/nosql-11.2.2.0/GettingStartedGuide/schemaevolution.html)","type":"Mdx","contentDigest":"a108698685bc0bcd79111afa5e33ab75","counter":261,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Apache Avro","description":"Apache Avro"},"exports":{},"rawBody":"---\ntitle: Apache Avro\ndescription: Apache Avro\n---\n\n## Introduction\n\nHere we explain the [Apache Avro](https://avro.apache.org/) messaging integration we have done in one of our integration tests for the **refarch-kc-container-ms** component, which is part of the [Reefer Containers reference implementation](https://ibm-cloud-architecture.github.io/refarch-kc/) of the [IBM Event Driven Architectures reference architecture](https://ibm-cloud-architecture.github.io/refarch-eda/). The Reefer Containers reference implementation is a simulation of what a container shipment process could look like in reality. From a manufacturer creating some goods to the delivery of those to a retailer, going through requesting a container, loading the goods into the container, finding a voyage for that container on a ship, monitoring the container's temperature and GPS location, delivering the container, unloading the goods, etc. As you can imagine, this scenario is ideal for an Event Driven architecture where we not only have a microservices based application but also the integration of these using Event Driven Architecture components (such as Kafka) and patterns (such as Saga, CQRS, Event Sourcing, etc).\n\n### What is Apache Avro\n\nAvro is an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.\n\n### Why Apache Avro\n\nThere are several websites that discuss the Apache Avro data serialization system benefits over other messaging data protocols. A simple google search will list dozens of them. Here, we will highlight just a few from a [Confluent blog post](https://www.confluent.io/blog/avro-kafka-data/):\n\n- It has a direct mapping to and from JSON\n- It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.\n- It is very fast.\n- It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.\n- It has a rich, extensible schema language defined in pure JSON\n- It has the best notion of compatibility for evolving your data over time.\n\n## Avro, Kafka and Schema Registry\n\nAvro relies on schemas. When Avro data is produced or read, the Avro schema for such piece of data is always present. This permits each datum to be written with no per-value overheads, making serialization both fast and small. An Avro schema defines the structure of the Avro data format. Schema Registry defines a scope in which schemas can evolve, and that scope is the subject. The name of the subject depends on the configured subject name strategy, which by default is set to derive subject name from topic name.\n\nIn our case, this Avro data are messages sent to a kafka topic. Each message is a key-value pair. Either the message key or the message value, or both, can be serialized as Avro. Integration with Schema Registry means that Kafka messages do not need to be written with the entire Avro schema. Instead, Kafka messages are written with the **schema id**. The producers writing the messages and the consumers reading the messages must be using the same Schema Registry to get the same mapping between a schema and schema id.\n\nKafka is used as Schema Registry storage backend. The special Kafka topic `<kafkastore.topic>` (default `_schemas`), with a single partition, is used as a highly available write ahead log. All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log. A Schema Registry instance therefore both produces and consumes messages under the `_schemas` topic. It produces messages to the log when, for example, new schemas are registered under a subject, or when updates to compatibility settings are registered. Schema Registry consumes from the `_schemas` log in a background thread, and updates its local caches on consumption of each new `_schemas` message to reflect the newly added schema or compatibility setting. Updating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability.\n\n### How does it all work\n\n![schema registry management](images/schema-registry-and-kafka.png)\n\nWhen the producer sends a message/event to a Kafka topic for the first time, it sends the schema for that message/event to the Schema Registry. The Schema Registry registers this schema to the subject for the Kafka topic we want to send the message/event to, and returns the **schema id** to the producer. The producer **caches this mapping between the schema and schema id** for subsequent message writes, so **it only contacts Schema Registry on the first message/event write** (unless the schema has changed, that is evolved, when the schema registry will be contacted again for validation and storage of this new version of the schema). Kafka messages are written along with the **schema id** rather than with the entire data schema.\n\nWhen a consumer reads this data, it sees the **Avro schema id and sends a schema request to Schema Registry**. Schema Registry retrieves the schema associated to that schema id, and **returns the schema to the consumer**. The consumer **caches** this mapping between the schema and schema id for subsequent message reads, so it only contacts Schema Registry on the first schema id read.\n\n## Our implementation\n\nAs mentioned in the introduction, the integration of the Apache Avro data serialization system has been done in one of the integration test for the `refarch-kc-container-ms` component of the [Reefer Containers reference implementation](https://ibm-cloud-architecture.github.io/refarch-kc/) of the [IBM Event Driven Architectures reference architecture](https://ibm-cloud-architecture.github.io/refarch-eda/).\n\nThe **refarch-kc-container-ms** component will take care of the reefer containers status. From adding new reefers to the available containers list to assigning a container to a particular order and managing the status of that reefer throughout the shipment process aforementioned. \n\nThe integration tests for our Reefer Containers reference implementation can be found [here](https://github.com/ibm-cloud-architecture/refarch-kc/tree/master/itg-tests). The integration tests are being developed in python and their main goal is to validate the successful deployment of the Reefer Containers reference implementation end-to-end.\n\nThe particular integration test (still under development) where we have integrated the Apache Avro serialization system can be found under the `ContainersPython` folder. More precisely, these are the files and folders involved in our implementation:\n\n```shell\n├── data_schemas/avro_test\n│   ├── container_event.avsc\n│   ├── container_event_key.avsc\n│   ├── container_event_payload.avsc\n│   ├── container_event_type.avsc\n│   ├── container_status.avsc\n│   └── utils\n│       └── avroEDAUtils.py\n└── itg-tests\n    ├── ContainersPython\n    │   ├── ConsumeAvroContainers.py\n    │   └── ContainerAvroProducer.py\n    └── kafka\n        ├── KcAvroConsumer.py\n        └── KcAvroProducer.py\n```\n\nthat will allow us to send **container events** into the **containers** Kafka topic and read from such topic.\n\nBy using these python scripts, we will be able to validate:\n\n- Sending/Receiving **Apache Avro encoded messages**.\n- **Apache Avro data schema definitions** for data correctness.\n- **Schema Registry** for Apache Avro data schema management.\n\n### Data Schemas\n\nAvro schemas are defined with JSON. An example of a Container Event for creating a new reefer container to the available list of containers for our reference application looks like:\n\n```json\n{\n  \"containerID\": \"container01\",\n  \"timestamp\": 1569410690,\n  \"type\": \"ContainerAdded\",\n  \"payload\": {\n    \"containerID\": \"container01\",\n    \"type\": \"Reefer\",\n    \"status\": \"Empty\",\n    \"latitude\": 37.8,\n    \"longitude\": -122.25,\n    \"capacity\": 110,\n    \"brand\": \"itg-brand\"\n  }\n}\n```\n\nAn Avro schema could be a nested schema which allows us to have a smaller reusable data schemas to define bigger and more complex ones. This is the case for our Container Event data schema. For instance, the payload is defined on its own data schema (`container_event_payload.avsc`) which the Container Event data schema refers to:\n\n```json\n{\n  \"namespace\": \"ibm.eda.kc.container.event\",\n  \"name\": \"payload\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"containerID\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"type\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"status\",\n      \"type\": \"ibm.eda.kc.container.status\"\n    },\n    {\n      \"name\": \"latitude\",\n      \"type\": \"float\"\n    },\n    {\n      \"name\": \"longitude\",\n      \"type\": \"float\"\n    },\n    {\n      \"name\": \"capacity\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"brand\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\nAs you can see, the status attribute of the payload is yet another data schema itself which, in this case, is of type enum:\n\n```json\n{\n  \"namespace\": \"ibm.eda.kc.container\",\n  \"name\": \"status\",\n  \"type\": \"enum\",\n  \"symbols\": [\n    \"Loaded\",\n    \"Empty\",\n    \"Unassignable\",\n    \"PartiallyLoaded\"\n  ]\n}\n```\n\nAll the different data schemas for a Container Event can be found under the `data_schemas/avro_test` folder.\n\nIn that folder we have also developed a util script (`avroEDAUtils.py`) to be able to construct the final Container Event data schema that is needed by our producer:\n\n```python\ndef getContainerEventSchema(schema_files_location):\n  # Read all the schemas needed in order to produce the final Container Event Schema\n  known_schemas = avro.schema.Names()\n  container_status_schema = LoadAvsc(schema_files_location + \"container_status.avsc\", known_schemas)\n  container_event_payload_schema = LoadAvsc(schema_files_location + \"container_event_payload.avsc\", known_schemas)\n  container_event_type_schema = LoadAvsc(schema_files_location + \"container_event_type.avsc\", known_schemas)\n  container_event_schema = LoadAvsc(schema_files_location + \"container_event.avsc\", known_schemas)\n  return container_event_schema\n\ndef LoadAvsc(file_path, names=None):\n  # Load avsc file\n  # file_path: path to schema file\n  # names(optional): avro.schema.Names object\n  file_text = open(file_path).read()\n  json_data = json.loads(file_text)\n  schema = avro.schema.SchemaFromJSONData(json_data, names)\n  return schema\n```\n\n#### See it in action\n\nHere, we are going to see how data schemas help with data correctness. Using the payload for our container messages/events as the example, this is the output of a correct message being sent:\n\n```\n--- Container event to be published: ---\n{\n  \"containerID\": \"container01\",\n  \"type\": \"Reefer\",\n  \"status\": \"Empty\",\n  \"latitude\": 37.8,\n  \"longitude\": -122.25,\n  \"capacity\": 110,\n  \"brand\": \"itg-brand\"\n}\n\nMessage delivered to containers [0]\n```\n\nHowever, if we try to send a payload where, for instance the container ID is an integer rather than a string, we will get an `avro.io.AvroTypeException`:\n\n```\navro.io.AvroTypeException: The datum {'containerID': 12345, 'type': 'Reefer', 'status': 'Empty', 'latitude': 37.8, 'longitude': -122.25, 'capacity': 110, 'brand': 'itg-brand'} is not an example of the schema\n{\n    \"type\": {\n        \"type\": \"record\",\n        \"name\": \"payload\",\n        \"namespace\": \"ibm.eda.kc.container.event\",\n        \"fields\": [\n          {\n            \"type\": \"string\",\n            \"name\": \"containerID\"\n          },\n          {\n            \"type\": \"string\",\n            \"name\": \"type\"\n          },\n          {\n            \"type\": {\n              \"type\": \"enum\",\n              \"name\": \"status\",\n              \"namespace\": \"ibm.eda.kc.container\",\n              \"symbols\": [\n                \"Loaded\",\n                \"Empty\",\n                \"Unassignable\",\n                \"PartiallyLoaded\"\n              ]\n            },\n            \"name\": \"status\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"latitude\"\n          },\n          {\n            \"type\": \"float\",\n            \"name\": \"longitude\"\n          },\n          {\n            \"type\": \"int\",\n            \"name\": \"capacity\"\n          },\n          {\n            \"type\": \"string\",\n            \"name\": \"brand\"\n          }\n        ]\n      }\n```\n\n### Producer and Consumer\n\nThe python scripts developed to implement a producer and consumer to a kafka topic that sends Avro messages whose data schemas are managed by a schema registry are:\n\n```shell\n└── itg-tests\n    ├── ContainersPython\n    │   ├── ConsumeAvroContainers.py\n    │   └── ContainerAvroProducer.py\n    └── kafka\n        ├── KcAvroConsumer.py\n        └── KcAvroProducer.py\n```\n\nWe have used the **confluent_kafka avro libraries** to implement our producer and consumer.\n\n```python\nfrom confluent_kafka.avro import AvroProducer, AvroConsumer\n```\n\n#### Producer\n\nWe create our `KafkaProducer` object where we define some of the **AvroProducer** options such as the schema registry url for data schema registration and management. But it is not until we call the `prepareProducer` method that we actually create the **AvroProducer** with that **schema registry** to be used as well as the **data schemas for the key and value** of our Container Event to be sent.\n\nFinally, in the `publishEvent` method we send a value plus a key to a kafka topic.\n\nproducer when we call `prepareProducer`\n\n```python\nimport json\nfrom confluent_kafka import KafkaError\nfrom confluent_kafka.avro import AvroProducer\n\nclass KafkaProducer:\n\n    def __init__(self,kafka_brokers = \"\",kafka_apikey = \"\",schema_registry_url = \"\"):\n        self.kafka_brokers = kafka_brokers\n        self.kafka_apikey = kafka_apikey\n        self.schema_registry_url = schema_registry_url\n\n    def prepareProducer(self,groupID = \"pythonproducers\",key_schema = \"\", value_schema = \"\"):\n        options ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'schema.registry.url': self.schema_registry_url,\n                'group.id': groupID\n        }\n        self.producer = AvroProducer(options,default_key_schema=key_schema,default_value_schema=value_schema)\n\n    def publishEvent(self, topicName, value, key):\n        # Important: value DOES NOT come in JSON format from ContainerAvroProducer.py. Therefore, we must convert it to JSON format first\n        self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(value)[key], callback=self.delivery_report)\n        self.producer.flush()\n```\n\nTo use this class you need to do the following steps:\n\n```python\n# load schema definitions for key and value\nfrom utils.avroEDAUtils import getContainerEventSchema, getContainerKeySchema\ncontainer_event_value_schema = getContainerEventSchema(\"/data_schemas/avro_test/\")\ncontainer_event_key_schema = getContainerKeySchema(\"/data_schemas/avro_test/\")\n# Create a producer with the schema registry URL end point\nkp = KafkaProducer(KAFKA_ENV,KAFKA_BROKERS,KAFKA_APIKEY,SCHEMA_REGISTRY_URL)\nkp.prepareProducer(\"ContainerProducerPython\",container_event_key_schema,container_event_value_schema)\n# loop on publishing events\nkp.publishEvent(TOPIC_NAME,container_event)\n```\n\n#### Consumer\n\nSimilarly to the producer, when we create a `KafkaConsumer` object we are just setting some of its attributes such as the kafka topic we will listen to and the schema registry url the producer will retrieve the data schemas from based on the schema ids messages comes with. It is only when we call the `prepareConsumer` method that we actually create the **AvroConsumer** and subscribe it to the intended kafka topic.\n\n```python\nimport json\nfrom confluent_kafka.avro import AvroConsumer\n\nclass KafkaConsumer:\n\n    def __init__(self, kafka_brokers = \"\", kafka_apikey = \"\", topic_name = \"\", schema_registry_url = \"\"):\n        self.kafka_brokers = kafka_brokers\n        self.kafka_apikey = kafka_apikey\n        self.topic_name = topic_name\n        self.schema_registry_url = schema_registry_url \n\n    def prepareConsumer(self, groupID = \"pythonconsumers\"):\n        options ={\n                'bootstrap.servers':  self.kafka_brokers,\n                'group.id': groupID,\n                'auto.offset.reset': 'earliest',\n                'schema.registry.url': self.schema_registry_url,\n        }\n        self.consumer = AvroConsumer(options)\n        self.consumer.subscribe([self.topic_name])\n\n# ...\n```\n\n### Schema registry\n\nFor now, we have used the [Confluent schema registry](https://hub.docker.com/r/confluentinc/cp-schema-registry/) for our work although our goal is to use [IBM Event Streams](https://ibm.github.io/event-streams/schemas/overview/).\n\nThe integration of the schema registry with your kafka broker is quite easy. In fact, all you need is to provide the schema registry with your zookeeper cluster url and give your schema registry a hostname: https://docs.confluent.io/current/installation/docker/config-reference.html#schema-registry-configuration\n\nOnce you have your schema registry up and running, this provides a rich API endpoint to operate with: https://docs.confluent.io/current/schema-registry/using.html#common-sr-usage-examples\n\nFor example:\n\nLet's assume we have created a new kafka topic called `avrotest` for testing our work. And let's also assume we are sending persona messages/events whose data schema is the following:\n\n```json\n{\n    \"namespace\": \"avro.test\",\n    \"name\": \"persona\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        }\n    ]\n }\n```\n\n- Get the **subjects** (that is, the kafka topics to which we have a schema registered against. As explained before, we either have registered the schema manually ourselves or the Avro producer has registered it when we have sent the first message)\n\n  ```http\n  curl -X GET http://localhost:8081/subjects\n\n  [\"avrotest-value\",\"avrotest-key\"]\n  ```\n\n- Get versions for a subject:\n\n  ```http\n  curl -X GET http://localhost:8081/subjects/avrotest-value/versions\n\n  [1]\n  ```\n\n- Get a specific version:\n\n  ```http\n  curl -X GET http://localhost:8081/subjects/avrotest-value/versions/1/\n\n  {\n    \"subject\": \"avrotest-value\",\n    \"version\": 1,\n    \"id\": 1,\n    \"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"persona\\\",\\\"namespace\\\":\\\"avro.test\\\",\\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"}]}\"\n  }\n  ```\n\n- Get the schema of a specific subject version:\n\n  ```http\n  curl -X GET http://localhost:8081/subjects/avrotest-value/versions/1/schema\n\n  {\n    \"type\": \"record\",\n    \"name\": \"persona\",\n    \"namespace\": \"avro.test\",\n    \"fields\": [\n      {\n        \"name\": \"name\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"age\",\n        \"type\": \"int\"\n      },\n      {\n        \"name\": \"gender\",\n        \"type\": \"string\"\n      }\n    ]\n  }\n  ```\n\n- Get the schema of a specific subject latest version:\n\n  ```http\n  curl -X GET http://localhost:8081/subjects/avrotest-value/versions/latest/schema\n\n  {\n    \"type\": \"record\",\n    \"name\": \"persona\",\n    \"namespace\": \"avro.test\",\n    \"fields\": [\n      {\n        \"name\": \"name\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"age\",\n        \"type\": \"int\"\n      },\n      {\n        \"name\": \"gender\",\n        \"type\": \"string\"\n      }\n    ]\n  }\n  ```\n\n### Data evolution\n\nSo far we  have seen what Avro is, what a data schema is, what a schema registry is and how this all works together. From creating a data schema for your messages/events to comply with to how the schema registry and data schemas work together. And we have also seen the code for doing all this, from the python code to send and receive Avro encoded messages based on their schemas to the rich API the Confluent schema registry provides to interact with.\n\nHowever, we have said little about the need for data to evolve. When you design an Event Driven architecture for your application (by applying [Event Storming](https://ibm-cloud-architecture.github.io/refarch-eda/methodology/eventstorming/) or [Domain Driven Design](https://ibm-cloud-architecture.github.io/refarch-eda/methodology/ddd/) for example), it is very hard to come up with data structures/schemas that will not need to evolve/change in time. That is, your data like your use or business cases may need to evolve. As a result, Avro data schemas must be somehow flexible to allow your data to evolve along with your application and use cases.\n\nBut it is not as easy as adding or removing data that travels in your events/messages or modifying the type of such data. And one of the reasons for this is that Kafka (or any other type of event broker) is many times used as the source of truth. That is, a place that you can trust as to what has happened. Hence, Kafka will serve as the event source of truth where all the events (that is, data) that happened (which could be bank transactions, communications, etc) will get stored (sometimes up to [hundreds of years](https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/)) and will be able to be replayed if needed. As a result, there must be a data schema management and data schema evolution put in place that allow the compatibility of old and new data schemas and, in fact, data at the end of the day.\n\nThere are mainly three types of data compatibility:\n\n1. Backward\n2. Forward\n3. Full\n\n#### Backward compatibility\n\nBackward compatibility means that **consumers using the new schema can read data produced with the last schema**.\n\nUsing the persona data schema already mentioned throughout this readme, what if we decide to change the data schema to add a new attribute such as place of birth? That is, the new schema would look like:\n\n```json\n{\n    \"namespace\": \"avro.test\",\n    \"name\": \"persona\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"place_of_birth\",\n            \"type\" : \"string\"\n        }\n    ]\n}\n```\n\nhere is the output when we try to produce an event/message with the above data schema:\n\n```python\n### Persona event to be published: ###\n{'name': 'david', 'age': '25', 'gender': 'male', 'place_of_birth': 'USA'}\n######################################\nTraceback (most recent call last):\n  File \"ContainerAvroProducer.py\", line 73, in <module>\n    kp.publishEvent(TOPIC_NAME,container_event,\"1\")\n  File \"/home/kafka/KcAvroProducer.py\", line 42, in publishEvent\n    self.producer.produce(topic=topicName,value=json.loads(value),key=json.loads(key), callback=self.delivery_report)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\", line 80, in produce\n    value = self._serializer.encode_record_with_schema(topic, value_schema, value)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 105, in encode_record_with_schema\n    schema_id = self.registry_client.register(subject, schema)\n  File \"/root/.local/lib/python3.7/site-packages/confluent_kafka/avro/cached_schema_registry_client.py\", line 223, in register\n    raise ClientError(\"Invalid Avro schema:\" + str(code))\nconfluent_kafka.avro.error.ClientError: Invalid Avro schema:422\n```\n\nAnd the reason for such error is that, because new schemas must be backward compatible (default compatibility mode for Confluent kafka data schemas topics), we can't just simply add a new attribute. Consumers using the new schema must be able to read data produced with the last schema. That is, if a consumer was to read old messages with the schema above, it would expect the `place_of_birth` attribute and its value on these old messages. However, the messages were produced with the old schema that did not enforce such attribute. Hence, the problem.\n\nWe can also check the compatibility of this new schema using the API:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\"}]}\"}' \n        http://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":false}\n```\n\nHow do we evolve our schema to add new attributes in a way that the schema is _BACKWARD_ compatible? Adding a **default** value for such attribute so the consumer can use it when reading old messages that were produced without that attribute:\n\n```json\n{\n    \"namespace\": \"avro.test\",\n    \"name\": \"persona\",\n    \"type\": \"record\",\n    \"fields\" : [\n        {\n            \"name\" : \"name\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"age\",\n            \"type\" : \"int\"\n        },\n        {\n            \"name\" : \"gender\",\n            \"type\" : \"string\"\n        },\n        {\n            \"name\" : \"place_of_birth\",\n            \"type\" : \"string\",\n            \"default\": \"nonDefined\"\n        }\n    ]\n}\n```\n\nRather than changing it straight in the code, we can do some sort of validation through the API:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" \n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nWe now can evolve our data schema to enforce a new attribute with our new messages/events being produced but making sure the consumer is able to read old messages that do not contain such attribute. We do so by sending a new persona event/message along with this new data schema. This will make the schema registry to register the new data schema.\n\nWe can validate the new data schema version has been registered by using the schema registry API:\n\n```http\n\ncurl -X GET http://localhost:8081/subjects/avrotest-value/versions\n\n[1,2]\n\ncurl -X GET http://localhost:8081/subjects/avrotest-value/versions/latest/schema\n\n{\"type\":\"record\",\n \"name\":\"persona\",\n \"namespace\":\"avro.test\",\n \"fields\":[{\"name\":\"name\",\"type\":\"string\"},\n           {\"name\":\"age\",\"type\":\"int\"},\n           {\"name\":\"gender\",\"type\":\"string\"},\n           {\"name\":\"place_of_birth\",\"type\":\"string\",\"default\":\"nonDefined\"}]}\n```\n\nWhat if we want to remove an attribute from our persona events/messages now? Well, this one is easy since the Avro consumer will simply ignore/drop all those attributes in the old persona events/messages that are not defined in the new data schema and just take in those that are defined.\n\nLet's try to remove the `gender` attribute:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\n#### Forward compatibility\n\nForwards compatibility means that **data produced with a new schema can be read by consumers using the last schema**.\n\nFirst, let's set the compatibility type to _FORWARD_ (default compatibility mode in Confluent kafka is backward):\n\n```http\ncurl -X PUT -H \"Content-Type: application/vnd.schemaregistry.v1+json\" --data '{\"compatibility\": \"FORWARD\"}' http://localhost:8081/config\n\n{\"compatibility\":\"FORWARD\"}\n\n```\n\n```http\ncurl -X GET http://localhost:8081/config\n\n{\"compatibilityLevel\":\"FORWARD\"}\n```\n\nNow, how about removing an attribute when the compatibility type configured is set to _FORWARD_? In this case, it is not as simple as removing the attribute from the new schema as the consumer will expect such attribute that the producer will not add to the events/messages. Let's try to remove the `gender` attribute from the persona messages/events:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":false}\n```\n\nSo, how can we produce new persona events/messages (without the `gender` attribute) that are compatible with the last data schema used by consumers (that expects an attribute called gender)?\n\nThe trick here is to **first register an \"intermediate\" data schema that adds a default value** to `gender` if it is not defined. This way, the \"intermediate\" data schema will become the last data schema for the consumers and when we producer sent messages that do not contain the `gender` attribute, the consumer will know what to do:\n\nIntermediate schema:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"gender\\\",\\\"type\\\": \\\"string\\\",\\\"default\\\": \\\"nonProvided\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nWe register this data schema either by sending it along with a message/event using our producer or we simply register it using the schema registry API. Once we have this \"intermediate\" schema registered that will actually become the last data schema for the consumer, we check if our end goal data schema without the `gender` attribute is forward compatible or not:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nIf we send a persona message that does not contain the `gender` attribute now, we should succeed:\n\n```python\n### Persona event to be published: ###\n{'name': 'david', 'age': 25, 'place_of_birth': 'USA'}\n######################################\nMessage delivered to avrotest [0]\n```\n\nContrary to the backward compatibility, in forward compatibility, adding a new attribute to your events/messages is not a problem because the consumers will simply ignore/drop this new attribute since the schema they are still using (the last one) does not include it. So let's say we want to add a new attribute called `hair` to represent the color of a persona's hair:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"place_of_birth\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"nonDefined\\\"},\n                                 {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"}]}\"}'\nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nWe see there is no problem at all and if we try to send a message/event containing this new attribute along with the new schema:\n\n```python\n### Persona event to be published: ###\n{'name': 'John', 'age': 25, 'place_of_birth': 'London', 'hair': 'brown'}\n######################################\nMessage delivered to avrotest [0]\n```\n\nThe new data schema is registered and new messages/events complying with that new data schema are sent with no problem at all.\n\n#### Full compatibility\n\nFull compatibility means **data schemas are both backward and forward compatible**. Data schemas evolve in a fully compatible way: **old data can be read with the new data schema, and new data can also be read with the last data schema**.\n\nIn some data formats, such as JSON, there are no full-compatible changes. Every modification is either only forward or only backward compatible. But in other data formats, like Avro, you can define fields with default values. In that case adding or removing a field with a default value is a fully compatible change.\n\nSo let's see if we can delete the `place_of_birth` attribute, the only attribute in our data schema that defines a default value:\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"}]}\"}' \nhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nIt looks like it may work. Let's send a message without that attribute along with the new data schema:\n\n```python\n### Persona event to be published: ###\n{'name': 'John', 'age': 25, 'hair': 'brown'}\n######################################\nMessage delivered to avrotest [0]\n```\n\nLet's now try to add an attribute with a default value. Let's say we want to add an attribute for the `hobbies` of a persona whose default value will be `none`\n\n```http\ncurl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\"\n--data '{\"schema\": \"{\\\"type\\\":\\\"record\\\",\n                     \\\"name\\\":\\\"persona\\\",\n                     \\\"namespace\\\":\\\"avro.test\\\",\n                     \\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"age\\\",\\\"type\\\":\\\"int\\\"},\n                                 {\\\"name\\\":\\\"hair\\\",\\\"type\\\":\\\"string\\\"},\n                                 {\\\"name\\\":\\\"hobbies\\\",\\\"type\\\":\\\"string\\\", \\\"default\\\":\\\"none\\\"}]}\"}'\nßhttp://localhost:8081/compatibility/subjects/avrotest-value/versions/latest\n\n{\"is_compatible\":true}\n```\n\nLet's send a message along with the new data schema to be completely sure:\n\n```python\n### Persona event to be published: ###\n{'name': 'John', 'age': 25, 'hair': 'brown', 'hobbies': 'dance,music,food'}\n######################################\nMessage delivered to avrotest [0]\n```\n\nAs expected, it did work.\n\nWe now know how a data schema can evolve when full compatibility is required. That is, we know what attributes can be removed and how to add new attributes.\n\n## Compendium\n\nHere are some links we have visited to carry out our work and found interesting to read:\n\n* [https://www.confluent.io/blog/avro-kafka-data/](https://www.confluent.io/blog/avro-kafka-data/)\n\n* [https://avro.apache.org/docs/current/](https://avro.apache.org/docs/current/)\n\n* [https://docs.confluent.io/current/schema-registry/index.html](https://docs.confluent.io/current/schema-registry/index.html)\n\n* [https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html#schema-registry-tutorial-definition](https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html#schema-registry-tutorial-definition)\n\n* [https://docs.oracle.com/database/nosql-11.2.2.0/GettingStartedGuide/schemaevolution.html](https://docs.oracle.com/database/nosql-11.2.2.0/GettingStartedGuide/schemaevolution.html)","fileAbsolutePath":"/home/runner/work/refarch-kc/refarch-kc/docs-gatsby/src/pages/avro/avro.mdx"}}}}